{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5583e3a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/angelajmzhou/ecs-171/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5dd47",
   "metadata": {
    "id": "04a5dd47"
   },
   "source": [
    "Lab1: Complete the TODO parts in the following code.\n",
    "- Using California Housing Dataset from sklearn, select input attributes 1,3,4  as the input features.\n",
    "- Using K-fold cross validation technique (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html), complete the implementation to train a regression model and report performance merics when asked in the following code.\n",
    "- For multiple degrees of model complexity (i.e., degree of polynomial in this exercise) in a for-loop, obtain the model with the minimum reducible_error, polynomial degree, and run the obtained model on the test data. For this part,you should use the split the data into train and test by [75:25] rate and report mse of the final model on test data.\n",
    "- Analyse the results of model performance according to different degrees of polynomial and the number of folds used. You can manipulate the code and share your analysis in terms of the performance of the model (mse and total error), such as for instnace which degree of the model complexity (in relation to the polynomial order) would give a better model? Feel free to include other analysis about the generated models in relation to their performance results. You can event plot the results to support your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04a564d5-48eb-4e4d-98c9-9a64c36f8bb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "589fec24",
    "outputId": "5b0ab926-8765-4957-8a44-48e281b7d17f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: 0.6159, Bias2: 1.1592, Total error: 1.7751\n",
      "Variance: 0.6857, Bias2: 1.2147, Total error: 1.9004\n",
      "Variance: 0.8001, Bias2: 1.4449, Total error: 2.2450\n",
      "Variance: 0.4341, Bias2: 1.1848, Total error: 1.6190\n",
      "Variance: 0.7587, Bias2: 1.4722, Total error: 2.2309\n",
      "model number: 3.000000 total error :1.618951\n",
      "Best Model Values: Variance: 0.7587, Bias2: 0.6617\n",
      "Variance: 1.4212, Bias2: 1.1390, Total error: 2.5602\n",
      "Variance: 0.6013, Bias2: 1.1999, Total error: 1.8012\n",
      "Variance: 0.8027, Bias2: 1.4435, Total error: 2.2462\n",
      "Variance: 0.5257, Bias2: 1.1828, Total error: 1.7085\n",
      "Variance: 0.8059, Bias2: 1.4748, Total error: 2.2808\n",
      "model number: 3.000000 total error :1.708497\n",
      "Best Model Values: Variance: 0.8059, Bias2: 0.7136\n",
      "Variance: 11.4957, Bias2: 1.1280, Total error: 12.6237\n",
      "Variance: 0.5895, Bias2: 1.1955, Total error: 1.7849\n",
      "Variance: 0.9781, Bias2: 1.4401, Total error: 2.4183\n",
      "Variance: 0.5374, Bias2: 1.1802, Total error: 1.7175\n",
      "Variance: 0.8490, Bias2: 1.4749, Total error: 2.3240\n",
      "model number: 3.000000 total error :1.717509\n",
      "Best Model Values: Variance: 0.8490, Bias2: 0.7361\n",
      "Variance: 1749.0026, Bias2: 1.3864, Total error: 1750.3890\n",
      "Variance: 0.5950, Bias2: 1.1899, Total error: 1.7849\n",
      "Variance: 1.0413, Bias2: 1.4397, Total error: 2.4810\n",
      "Variance: 0.5600, Bias2: 1.1775, Total error: 1.7376\n",
      "Variance: 0.8850, Bias2: 1.4750, Total error: 2.3600\n",
      "model number: 3.000000 total error :1.737571\n",
      "Best Model Values: Variance: 0.8850, Bias2: 0.7449\n",
      "Variance: 421853.5244, Bias2: 189.4010, Total error: 422042.9254\n",
      "Variance: 0.6051, Bias2: 1.1880, Total error: 1.7931\n",
      "Variance: 1.2661, Bias2: 1.4394, Total error: 2.7055\n",
      "Variance: 1.1446, Bias2: 1.1801, Total error: 2.3247\n",
      "Variance: 0.9058, Bias2: 1.4742, Total error: 2.3800\n",
      "model number: 1.000000 total error :1.793138\n",
      "Best Model Values: Variance: 0.9058, Bias2: 0.7574\n",
      "Variance: 0.7159, Bias2: 0.9605, Total error: 1.6764\n",
      "Variance: 0.5295, Bias2: 1.2807, Total error: 1.8102\n",
      "Variance: 0.8720, Bias2: 1.6932, Total error: 2.5652\n",
      "Variance: 0.4601, Bias2: 0.7551, Total error: 1.2152\n",
      "Variance: 0.7946, Bias2: 1.7509, Total error: 2.5455\n",
      "Variance: 0.7886, Bias2: 1.1658, Total error: 1.9544\n",
      "Variance: 0.3192, Bias2: 0.7032, Total error: 1.0224\n",
      "Variance: 0.5002, Bias2: 1.4701, Total error: 1.9704\n",
      "Variance: 0.8356, Bias2: 1.4281, Total error: 2.2637\n",
      "Variance: 0.4273, Bias2: 0.8570, Total error: 1.2843\n",
      "model number: 6.000000 total error :1.022392\n",
      "Best Model Values: Variance: 0.4273, Bias2: 0.7334\n",
      "Variance: 2.2661, Bias2: 0.9596, Total error: 3.2257\n",
      "Variance: 0.6689, Bias2: 1.2433, Total error: 1.9122\n",
      "Variance: 0.7389, Bias2: 1.6719, Total error: 2.4108\n",
      "Variance: 0.4416, Bias2: 0.7586, Total error: 1.2003\n",
      "Variance: 0.7573, Bias2: 1.7491, Total error: 2.5064\n",
      "Variance: 0.8264, Bias2: 1.1704, Total error: 1.9969\n",
      "Variance: 0.4521, Bias2: 0.6752, Total error: 1.1273\n",
      "Variance: 0.5263, Bias2: 1.4629, Total error: 1.9892\n",
      "Variance: 0.7195, Bias2: 1.4036, Total error: 2.1231\n",
      "Variance: 0.5898, Bias2: 0.8340, Total error: 1.4238\n",
      "model number: 6.000000 total error :1.127293\n",
      "Best Model Values: Variance: 0.5898, Bias2: 0.8207\n",
      "Variance: 25.0727, Bias2: 0.9454, Total error: 26.0182\n",
      "Variance: 0.6731, Bias2: 1.2442, Total error: 1.9173\n",
      "Variance: 0.7162, Bias2: 1.6498, Total error: 2.3660\n",
      "Variance: 0.4483, Bias2: 0.7573, Total error: 1.2055\n",
      "Variance: 0.8145, Bias2: 1.7429, Total error: 2.5575\n",
      "Variance: 1.0222, Bias2: 1.1795, Total error: 2.2017\n",
      "Variance: 0.4487, Bias2: 0.6679, Total error: 1.1166\n",
      "Variance: 0.5575, Bias2: 1.4701, Total error: 2.0276\n",
      "Variance: 0.8072, Bias2: 1.3860, Total error: 2.1932\n",
      "Variance: 0.5494, Bias2: 0.8287, Total error: 1.3781\n",
      "model number: 6.000000 total error :1.116568\n",
      "Best Model Values: Variance: 0.5494, Bias2: 0.8526\n",
      "Variance: 963.4464, Bias2: 1.4386, Total error: 964.8850\n",
      "Variance: 0.6859, Bias2: 1.2419, Total error: 1.9278\n",
      "Variance: 0.7113, Bias2: 1.6341, Total error: 2.3454\n",
      "Variance: 0.4668, Bias2: 0.7571, Total error: 1.2239\n",
      "Variance: 0.8315, Bias2: 1.7403, Total error: 2.5718\n",
      "Variance: 1.0858, Bias2: 1.1823, Total error: 2.2681\n",
      "Variance: 0.4650, Bias2: 0.6472, Total error: 1.1123\n",
      "Variance: 0.5851, Bias2: 1.4718, Total error: 2.0570\n",
      "Variance: 0.8190, Bias2: 1.3749, Total error: 2.1939\n",
      "Variance: 0.5726, Bias2: 0.8258, Total error: 1.3985\n",
      "model number: 6.000000 total error :1.112269\n",
      "Best Model Values: Variance: 0.5726, Bias2: 0.8654\n",
      "Variance: 252972.2232, Bias2: 146.9534, Total error: 253119.1766\n",
      "Variance: 0.7376, Bias2: 1.2467, Total error: 1.9842\n",
      "Variance: 0.7293, Bias2: 1.6297, Total error: 2.3590\n",
      "Variance: 0.4660, Bias2: 0.7572, Total error: 1.2232\n",
      "Variance: 0.8602, Bias2: 1.7396, Total error: 2.5998\n",
      "Variance: 1.0835, Bias2: 1.1798, Total error: 2.2634\n",
      "Variance: 1.8967, Bias2: 0.6708, Total error: 2.5675\n",
      "Variance: 0.5887, Bias2: 1.4715, Total error: 2.0602\n",
      "Variance: 0.8222, Bias2: 1.3741, Total error: 2.1963\n",
      "Variance: 0.6824, Bias2: 0.8270, Total error: 1.5094\n",
      "model number: 3.000000 total error :1.223199\n",
      "Best Model Values: Variance: 0.6824, Bias2: 0.8653\n",
      "Variance: 0.5293, Bias2: 0.8119, Total error: 1.3412\n",
      "Variance: 0.7361, Bias2: 1.1656, Total error: 1.9017\n",
      "Variance: 0.6107, Bias2: 1.3862, Total error: 1.9969\n",
      "Variance: 1.1020, Bias2: 2.1048, Total error: 3.2068\n",
      "Variance: 0.5783, Bias2: 1.0628, Total error: 1.6411\n",
      "Variance: 0.2562, Bias2: 0.4051, Total error: 0.6613\n",
      "Variance: 0.9147, Bias2: 2.1507, Total error: 3.0654\n",
      "Variance: 0.8336, Bias2: 1.2210, Total error: 2.0546\n",
      "Variance: 0.5481, Bias2: 0.7990, Total error: 1.3471\n",
      "Variance: 0.3476, Bias2: 0.6633, Total error: 1.0109\n",
      "Variance: 0.3860, Bias2: 0.9016, Total error: 1.2876\n",
      "Variance: 0.5311, Bias2: 1.7546, Total error: 2.2857\n",
      "Variance: 0.7589, Bias2: 1.3438, Total error: 2.1027\n",
      "Variance: 0.8517, Bias2: 1.5498, Total error: 2.4015\n",
      "Variance: 0.4582, Bias2: 0.9105, Total error: 1.3686\n",
      "model number: 5.000000 total error :0.661323\n",
      "Best Model Values: Variance: 0.4582, Bias2: 0.7562\n",
      "Variance: 0.6637, Bias2: 0.8085, Total error: 1.4722\n",
      "Variance: 2.8090, Bias2: 1.1408, Total error: 3.9498\n",
      "Variance: 0.7349, Bias2: 1.3794, Total error: 2.1142\n",
      "Variance: 0.8809, Bias2: 2.0949, Total error: 2.9758\n",
      "Variance: 0.5185, Bias2: 1.0456, Total error: 1.5641\n",
      "Variance: 0.3119, Bias2: 0.4225, Total error: 0.7344\n",
      "Variance: 0.8256, Bias2: 2.1512, Total error: 2.9769\n",
      "Variance: 0.8023, Bias2: 1.2199, Total error: 2.0222\n",
      "Variance: 0.6636, Bias2: 0.7999, Total error: 1.4635\n",
      "Variance: 0.4766, Bias2: 0.6254, Total error: 1.1020\n",
      "Variance: 0.4822, Bias2: 0.9025, Total error: 1.3847\n",
      "Variance: 0.5445, Bias2: 1.7440, Total error: 2.2885\n",
      "Variance: 0.6567, Bias2: 1.3235, Total error: 1.9802\n",
      "Variance: 0.8700, Bias2: 1.5485, Total error: 2.4185\n",
      "Variance: 0.6292, Bias2: 0.8832, Total error: 1.5123\n",
      "model number: 5.000000 total error :0.734383\n",
      "Best Model Values: Variance: 0.6292, Bias2: 0.8433\n",
      "Variance: 0.6608, Bias2: 0.8077, Total error: 1.4684\n",
      "Variance: 54.9386, Bias2: 0.9917, Total error: 55.9303\n",
      "Variance: 0.7633, Bias2: 1.3802, Total error: 2.1435\n",
      "Variance: 0.8303, Bias2: 2.0546, Total error: 2.8849\n",
      "Variance: 0.5552, Bias2: 1.0498, Total error: 1.6050\n",
      "Variance: 0.3100, Bias2: 0.4185, Total error: 0.7285\n",
      "Variance: 0.8896, Bias2: 2.1355, Total error: 3.0251\n",
      "Variance: 0.9122, Bias2: 1.2204, Total error: 2.1326\n",
      "Variance: 0.8277, Bias2: 0.8077, Total error: 1.6354\n",
      "Variance: 0.4688, Bias2: 0.6069, Total error: 1.0757\n",
      "Variance: 0.4949, Bias2: 0.8990, Total error: 1.3939\n",
      "Variance: 0.5702, Bias2: 1.7532, Total error: 2.3234\n",
      "Variance: 0.7023, Bias2: 1.3243, Total error: 2.0266\n",
      "Variance: 0.9704, Bias2: 1.5439, Total error: 2.5143\n",
      "Variance: 0.5803, Bias2: 0.8780, Total error: 1.4583\n",
      "model number: 5.000000 total error :0.728528\n",
      "Best Model Values: Variance: 0.5803, Bias2: 0.8796\n",
      "Variance: 0.7057, Bias2: 0.8080, Total error: 1.5137\n",
      "Variance: 2529.2312, Bias2: 1.1703, Total error: 2530.4015\n",
      "Variance: 0.7780, Bias2: 1.3807, Total error: 2.1588\n",
      "Variance: 0.8160, Bias2: 2.0333, Total error: 2.8493\n",
      "Variance: 0.5769, Bias2: 1.0487, Total error: 1.6256\n",
      "Variance: 0.3323, Bias2: 0.4179, Total error: 0.7502\n",
      "Variance: 0.8946, Bias2: 2.1277, Total error: 3.0224\n",
      "Variance: 0.9452, Bias2: 1.2214, Total error: 2.1666\n",
      "Variance: 1.0212, Bias2: 0.8066, Total error: 1.8278\n",
      "Variance: 0.5076, Bias2: 0.5801, Total error: 1.0877\n",
      "Variance: 0.5357, Bias2: 0.8934, Total error: 1.4290\n",
      "Variance: 0.5908, Bias2: 1.7539, Total error: 2.3447\n",
      "Variance: 0.7382, Bias2: 1.3164, Total error: 2.0546\n",
      "Variance: 0.9705, Bias2: 1.5426, Total error: 2.5131\n",
      "Variance: 0.6139, Bias2: 0.8746, Total error: 1.4885\n",
      "model number: 5.000000 total error :0.750224\n",
      "Best Model Values: Variance: 0.6139, Bias2: 0.8998\n",
      "Variance: 1.2656, Bias2: 0.8060, Total error: 2.0717\n",
      "Variance: 1084472.4058, Bias2: 775.7623, Total error: 1085248.1681\n",
      "Variance: 0.8530, Bias2: 1.3864, Total error: 2.2393\n",
      "Variance: 0.8396, Bias2: 2.0269, Total error: 2.8666\n",
      "Variance: 0.5843, Bias2: 1.0487, Total error: 1.6329\n",
      "Variance: 0.3299, Bias2: 0.4183, Total error: 0.7481\n",
      "Variance: 0.9097, Bias2: 2.1234, Total error: 3.0331\n",
      "Variance: 0.9801, Bias2: 1.2214, Total error: 2.2015\n",
      "Variance: 0.8333, Bias2: 0.7961, Total error: 1.6294\n",
      "Variance: 3.1440, Bias2: 0.6250, Total error: 3.7691\n",
      "Variance: 0.5267, Bias2: 0.8931, Total error: 1.4198\n",
      "Variance: 0.5989, Bias2: 1.7526, Total error: 2.3515\n",
      "Variance: 0.7386, Bias2: 1.3160, Total error: 2.0546\n",
      "Variance: 0.9766, Bias2: 1.5425, Total error: 2.5191\n",
      "Variance: 0.7735, Bias2: 0.8765, Total error: 1.6500\n",
      "model number: 5.000000 total error :0.748148\n",
      "Best Model Values: Variance: 0.7735, Bias2: 0.9023\n",
      "Variance: 0.5550, Bias2: 0.7820, Total error: 1.3370\n",
      "Variance: 0.8564, Bias2: 1.1017, Total error: 1.9582\n",
      "Variance: 0.2860, Bias2: 0.6357, Total error: 0.9217\n",
      "Variance: 0.6034, Bias2: 1.3025, Total error: 1.9059\n",
      "Variance: 0.5580, Bias2: 1.3972, Total error: 1.9552\n",
      "Variance: 0.9865, Bias2: 1.7405, Total error: 2.7270\n",
      "Variance: 0.6488, Bias2: 1.0108, Total error: 1.6596\n",
      "Variance: 0.2413, Bias2: 0.4065, Total error: 0.6479\n",
      "Variance: 0.8962, Bias2: 2.0275, Total error: 2.9236\n",
      "Variance: 0.6476, Bias2: 1.2618, Total error: 1.9094\n",
      "Variance: 0.7826, Bias2: 1.1416, Total error: 1.9241\n",
      "Variance: 0.5983, Bias2: 0.9156, Total error: 1.5139\n",
      "Variance: 0.3491, Bias2: 0.6656, Total error: 1.0147\n",
      "Variance: 0.2849, Bias2: 0.7117, Total error: 0.9966\n",
      "Variance: 0.4923, Bias2: 0.9561, Total error: 1.4484\n",
      "Variance: 0.4918, Bias2: 2.0544, Total error: 2.5462\n",
      "Variance: 0.8399, Bias2: 1.6645, Total error: 2.5043\n",
      "Variance: 0.7586, Bias2: 1.1565, Total error: 1.9151\n",
      "Variance: 0.3329, Bias2: 0.6327, Total error: 0.9656\n",
      "Variance: 0.5272, Bias2: 1.0724, Total error: 1.5995\n",
      "model number: 7.000000 total error :0.647850\n",
      "Best Model Values: Variance: 0.5272, Bias2: 0.7349\n",
      "Variance: 0.6525, Bias2: 0.7814, Total error: 1.4339\n",
      "Variance: 3.6890, Bias2: 1.0938, Total error: 4.7828\n",
      "Variance: 0.3914, Bias2: 0.4908, Total error: 0.8822\n",
      "Variance: 0.6839, Bias2: 1.3037, Total error: 1.9876\n",
      "Variance: 0.5062, Bias2: 1.3782, Total error: 1.8845\n",
      "Variance: 0.7946, Bias2: 1.7280, Total error: 2.5226\n",
      "Variance: 0.5621, Bias2: 1.0055, Total error: 1.5677\n",
      "Variance: 0.2988, Bias2: 0.4249, Total error: 0.7237\n",
      "Variance: 0.7608, Bias2: 2.0117, Total error: 2.7726\n",
      "Variance: 0.7066, Bias2: 1.2625, Total error: 1.9691\n",
      "Variance: 0.6743, Bias2: 1.1445, Total error: 1.8187\n",
      "Variance: 0.7325, Bias2: 0.9080, Total error: 1.6405\n",
      "Variance: 0.4795, Bias2: 0.6415, Total error: 1.1210\n",
      "Variance: 0.3993, Bias2: 0.6903, Total error: 1.0896\n",
      "Variance: 0.5203, Bias2: 0.9610, Total error: 1.4813\n",
      "Variance: 0.5143, Bias2: 2.0421, Total error: 2.5564\n",
      "Variance: 0.7205, Bias2: 1.6498, Total error: 2.3703\n",
      "Variance: 0.6300, Bias2: 1.1298, Total error: 1.7598\n",
      "Variance: 0.4585, Bias2: 0.6205, Total error: 1.0789\n",
      "Variance: 0.7085, Bias2: 1.0440, Total error: 1.7525\n",
      "model number: 7.000000 total error :0.723744\n",
      "Best Model Values: Variance: 0.7085, Bias2: 0.8120\n",
      "Variance: 0.6598, Bias2: 0.7814, Total error: 1.4413\n",
      "Variance: 50.1274, Bias2: 1.0288, Total error: 51.1563\n",
      "Variance: 0.3496, Bias2: 0.4942, Total error: 0.8438\n",
      "Variance: 0.7374, Bias2: 1.3038, Total error: 2.0412\n",
      "Variance: 0.4752, Bias2: 1.3535, Total error: 1.8287\n",
      "Variance: 0.8164, Bias2: 1.7167, Total error: 2.5331\n",
      "Variance: 0.5822, Bias2: 1.0095, Total error: 1.5917\n",
      "Variance: 0.2930, Bias2: 0.4222, Total error: 0.7152\n",
      "Variance: 0.8131, Bias2: 1.9914, Total error: 2.8046\n",
      "Variance: 0.7530, Bias2: 1.2626, Total error: 2.0156\n",
      "Variance: 0.8040, Bias2: 1.1485, Total error: 1.9525\n",
      "Variance: 0.9384, Bias2: 0.9222, Total error: 1.8606\n",
      "Variance: 0.4845, Bias2: 0.6251, Total error: 1.1096\n",
      "Variance: 0.3842, Bias2: 0.6898, Total error: 1.0739\n",
      "Variance: 0.5616, Bias2: 0.9555, Total error: 1.5171\n",
      "Variance: 0.5337, Bias2: 2.0492, Total error: 2.5829\n",
      "Variance: 0.7569, Bias2: 1.6560, Total error: 2.4129\n",
      "Variance: 0.7425, Bias2: 1.1106, Total error: 1.8532\n",
      "Variance: 0.4324, Bias2: 0.6165, Total error: 1.0488\n",
      "Variance: 0.6593, Bias2: 1.0378, Total error: 1.6970\n",
      "model number: 7.000000 total error :0.715238\n",
      "Best Model Values: Variance: 0.6593, Bias2: 0.8476\n",
      "Variance: 0.6659, Bias2: 0.7816, Total error: 1.4475\n",
      "Variance: 1852.3707, Bias2: 2.7435, Total error: 1855.1143\n",
      "Variance: 0.3531, Bias2: 0.4748, Total error: 0.8279\n",
      "Variance: 0.7224, Bias2: 1.3047, Total error: 2.0271\n",
      "Variance: 0.4913, Bias2: 1.3390, Total error: 1.8303\n",
      "Variance: 0.8065, Bias2: 1.7102, Total error: 2.5167\n",
      "Variance: 0.5971, Bias2: 1.0111, Total error: 1.6082\n",
      "Variance: 0.3161, Bias2: 0.4230, Total error: 0.7391\n",
      "Variance: 0.8158, Bias2: 1.9761, Total error: 2.7919\n",
      "Variance: 0.7767, Bias2: 1.2635, Total error: 2.0401\n",
      "Variance: 0.8161, Bias2: 1.1531, Total error: 1.9692\n",
      "Variance: 1.2077, Bias2: 0.9214, Total error: 2.1290\n",
      "Variance: 0.5430, Bias2: 0.6000, Total error: 1.1430\n",
      "Variance: 0.4000, Bias2: 0.6754, Total error: 1.0755\n",
      "Variance: 0.6064, Bias2: 0.9539, Total error: 1.5603\n",
      "Variance: 0.5475, Bias2: 2.0481, Total error: 2.5957\n",
      "Variance: 0.7907, Bias2: 1.6500, Total error: 2.4407\n",
      "Variance: 0.7252, Bias2: 1.1022, Total error: 1.8274\n",
      "Variance: 0.4263, Bias2: 0.6148, Total error: 1.0411\n",
      "Variance: 0.7161, Bias2: 1.0341, Total error: 1.7502\n",
      "model number: 7.000000 total error :0.739106\n",
      "Best Model Values: Variance: 0.7161, Bias2: 0.8659\n",
      "Variance: 0.6684, Bias2: 0.7817, Total error: 1.4501\n",
      "Variance: 501740.6695, Bias2: 541.4329, Total error: 502282.1024\n",
      "Variance: 0.3572, Bias2: 0.4720, Total error: 0.8292\n",
      "Variance: 0.7343, Bias2: 1.3051, Total error: 2.0394\n",
      "Variance: 0.5040, Bias2: 1.3352, Total error: 1.8392\n",
      "Variance: 0.8191, Bias2: 1.7079, Total error: 2.5270\n",
      "Variance: 0.5987, Bias2: 1.0113, Total error: 1.6100\n",
      "Variance: 0.3131, Bias2: 0.4235, Total error: 0.7366\n",
      "Variance: 0.8269, Bias2: 1.9693, Total error: 2.7962\n",
      "Variance: 0.8129, Bias2: 1.2643, Total error: 2.0772\n",
      "Variance: 0.8267, Bias2: 1.1543, Total error: 1.9810\n",
      "Variance: 0.9284, Bias2: 0.9079, Total error: 1.8364\n",
      "Variance: 4.0039, Bias2: 0.6559, Total error: 4.6598\n",
      "Variance: 0.3945, Bias2: 0.6739, Total error: 1.0684\n",
      "Variance: 0.6059, Bias2: 0.9539, Total error: 1.5598\n",
      "Variance: 0.5573, Bias2: 2.0465, Total error: 2.6038\n",
      "Variance: 0.7905, Bias2: 1.6488, Total error: 2.4393\n",
      "Variance: 0.7314, Bias2: 1.1020, Total error: 1.8334\n",
      "Variance: 0.4443, Bias2: 0.6145, Total error: 1.0588\n",
      "Variance: 0.8554, Bias2: 1.0384, Total error: 1.8938\n",
      "model number: 7.000000 total error :0.736618\n",
      "Best Model Values: Variance: 0.8554, Bias2: 0.8650\n"
     ]
    }
   ],
   "source": [
    "###### import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def polynomial_regression(degree, X, y, folds, test_size=0.25, random_state=None):\n",
    "    # Define number of folds for cross-validation\n",
    "    kf = KFold(folds)\n",
    "\n",
    "    # Initialize lists to store results for variance, bias2s, total_error, and models\n",
    "    # TODO\n",
    "    variance_list = []\n",
    "    bias2s_list = []\n",
    "    total_error = []\n",
    "    models = []\n",
    "\n",
    "    # Set the polynomial degree of the model\n",
    "    poly_features = PolynomialFeatures(degree)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X_poly):\n",
    "        # Split data into training and testing sets for this fold\n",
    "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit polynomial regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate variance and Bias^2 for this fold\n",
    "        variance = np.mean((np.mean(y_pred)-(y_pred))**2)\n",
    "        bias2 = np.mean(((np.mean(y_pred)- y_test)) ** 2)\n",
    "\n",
    "        # Append results to lists\n",
    "        variance_list.append(variance)\n",
    "        bias2s_list.append(bias2)\n",
    "        total_error.append((bias2+variance))\n",
    "        models.append(model)\n",
    "\n",
    "        # Print results  including variance, Bias^2 and the total_error for this fold.\n",
    "        print(\"Variance: {:.4f}, Bias2: {:.4f}, Total error: {:.4f}\".format(variance, bias2, bias2 + variance))\n",
    "\n",
    "    # print the total_error of the best model\n",
    "    min_error_index = np.argmin(total_error)\n",
    "    best_model = models[min_error_index]\n",
    "    print(\"model number: {:1f} total error :{:4f}\".format(min_error_index, total_error[min_error_index]))\n",
    "\n",
    "    # Testing the final model on the test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=42)\n",
    "    # Obtain the predictions on the test data\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    test_variance = np.mean(((y_pred)-np.mean(y_pred))**2)\n",
    "    test_bias2 = np.mean(((np.mean(y_pred)- y_pred_test)) ** 2)\n",
    "    print(\"Best Model Values: Variance: {:.4f}, Bias2: {:.4f}\".format(test_variance, test_bias2,))\n",
    "    # Calculate the mean squared error (MSE) on the test data\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    #also find error\n",
    "    return mse , best_model, total_error[min_error_index]\n",
    "\n",
    "# Example usage: load California Housing Dataset and select the first, third, and forth attributes as input features in X\n",
    "# TODO\n",
    "housing_data = fetch_california_housing()\n",
    "X = housing_data.data[:, [0, 2, 3]] #: means all rows, [0,2,3] means these specific columns\n",
    "y = housing_data.target\n",
    "\n",
    "degrees = range(1, 6)  # Try polynomial degrees from 1 to 5\n",
    "folds_range = range(5, 21, 5) #try fold #'s 5-20\n",
    "folds_mse_list = []\n",
    "degree_error_list = []\n",
    "# Loop over different folds and degrees\n",
    "for fold in folds_range:\n",
    "    fold_mse_list = []\n",
    "    for degree in degrees:\n",
    "        mse, best_model, total_error = polynomial_regression(degree, X, y, folds=fold, random_state=42)\n",
    "        degree_error_list.append({\"Fold\": fold, \"Degree\": degree, \"Total Error\": total_error, \"MSE\": mse})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6024d-1023-4c9b-abae-28b2528b3306",
   "metadata": {
    "id": "8ed6024d-1023-4c9b-abae-28b2528b3306"
   },
   "source": [
    "#ANALYSIS\n",
    "**Analyse the results of model performance according to different degrees of polynomial and the number of folds used. You can manipulate the code and share your analysis in terms of the performance of the model (mse and total error),\n",
    "such as for instnace which degree of the model complexity (in relation to the polynomial order) would give a better model? Feel free to include other analysis about the generated models in relation to their performance results.\n",
    "You can event plot the results to support your analysis.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05eeb83f-b422-4eb9-ba8c-292f8d47d0fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'polynomial_regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m folds_range:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m degree \u001b[38;5;129;01min\u001b[39;00m degrees:\n\u001b[0;32m----> 7\u001b[0m         mse, best_model, total_error \u001b[38;5;241m=\u001b[39m polynomial_regression(degree, X, y, folds\u001b[38;5;241m=\u001b[39mfold, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      8\u001b[0m         degree_error_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold\u001b[39m\u001b[38;5;124m\"\u001b[39m: fold, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDegree\u001b[39m\u001b[38;5;124m\"\u001b[39m: degree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Error\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_error, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: mse})\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Convert results to DataFrame for easy visualization\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'polynomial_regression' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "degrees = range(1, 6)  # Try polynomial degrees from 1 to 5\n",
    "folds_range = range(5, 21, 5) #try fold #'s 5-20\n",
    "degree_error_list = []\n",
    "# Loop over different folds and degrees\n",
    "for fold in folds_range:\n",
    "    for degree in degrees:\n",
    "        mse, best_model, total_error = polynomial_regression(degree, X, y, folds=fold, random_state=42)\n",
    "        degree_error_list.append({\"Fold\": fold, \"Degree\": degree, \"Total Error\": total_error, \"MSE\": mse})\n",
    "\n",
    "# Convert results to DataFrame for easy visualization\n",
    "degree_error_df = pd.DataFrame(degree_error_list)\n",
    "\n",
    "print(degree_error_df.head())\n",
    "\n",
    "# Plot MSE vs. Polynomial Degree\n",
    "plt.figure()\n",
    "for fold in folds_range:\n",
    "    fold_data = degree_error_df[degree_error_df[\"Fold\"] == fold]\n",
    "    plt.plot(fold_data[\"Degree\"], fold_data[\"MSE\"], marker='o', label=f'Fold {fold}')\n",
    "\n",
    "plt.title('MSE vs. Polynomial Degree')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Total Error vs. Polynomial Degree for each fold\n",
    "plt.figure()\n",
    "for fold in folds_range:\n",
    "    fold_data = degree_error_df[degree_error_df[\"Fold\"] == fold]\n",
    "    plt.plot(fold_data[\"Degree\"], fold_data[\"Total Error\"], marker='o', label=f'Fold {fold}')\n",
    "\n",
    "plt.title('Total Error vs. Polynomial Degree')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Total Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cTdTVOvC6dx",
   "metadata": {
    "id": "9cTdTVOvC6dx"
   },
   "source": [
    "One interesting pattern I observed with the default k-fold validation settings was that the first split in each degree had significantly higher variance and bias compared to subsequent splits, especially for higher-degree models. This pattern was likely due to the presence of outliers in the first fold or the model overfitting to the training set. When I set the shuffle parameter to True, however, this issue was remedied, as the outliers were evenly distributed across all folds. However, this resulted in a higher error throughout the other folds. To prevent the outliers from affecting all folds, I set shuffle back to False, although usually this wouldn't be the case, since outliers would typically be handled through proper exploratory data analysis (EDA).\n",
    "\n",
    "Analyzing the results with 10-fold validation, the best model in terms of mean squared error (MSE) was the polynomial model of degree 5 (MSE = 0.59), followed closely by the model of degree 4 (MSE = 0.60). However, when considering the total error (variance + bias squared), the degree 1 polynomial model had the lowest total error (1.1607). This highlights the trade-off between model complexity and overfitting. Higher-degree models perform better on MSE, they may not generalize as well, with more variance between predicted values and more variance between predicted and actual values when run on the test set. \n",
    "\n",
    "\n",
    "As seen from the plots above, MSE decreased with an increasing number of folds. More folds provide more data for training, leading to models which better represent the entire dataset. This pattern held for the total error as well, though there were diminishing returns beyond 15 folds, with 20 folds barely being an improvement over the previous iteration (red line [20 folds] vs. green line [15 folds]). Higher numbers of folds also seem to even out the differential in error between the higher-degree values; while with 5-10 folds, 5 degrees seems to be a significant improvement over 4 degrees, that distance in total error values is less pronounced when there are more folds. On the other hand, MSE clearly decreases with higher degrees, no matter the number of folds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128feaf-f909-4a00-ad3a-a2e49664883e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131d5c9-f3c4-45c2-a32f-7c2e96f379c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
