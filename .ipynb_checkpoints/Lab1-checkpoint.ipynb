{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5583e3a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/angelajmzhou/ecs-171/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5dd47",
   "metadata": {
    "id": "04a5dd47"
   },
   "source": [
    "Lab1: Complete the TODO parts in the following code.\n",
    "- Using California Housing Dataset from sklearn, select input attributes 1,3,4  as the input features.\n",
    "- Using K-fold cross validation technique (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html), complete the implementation to train a regression model and report performance merics when asked in the following code.\n",
    "- For multiple degrees of model complexity (i.e., degree of polynomial in this exercise) in a for-loop, obtain the model with the minimum reducible_error, polynomial degree, and run the obtained model on the test data. For this part,you should use the split the data into train and test by [75:25] rate and report mse of the final model on test data.\n",
    "- Analyse the results of model performance according to different degrees of polynomial and the number of folds used. You can manipulate the code and share your analysis in terms of the performance of the model (mse and total error), such as for instnace which degree of the model complexity (in relation to the polynomial order) would give a better model? Feel free to include other analysis about the generated models in relation to their performance results. You can event plot the results to support your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "589fec24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "589fec24",
    "outputId": "5b0ab926-8765-4957-8a44-48e281b7d17f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: 0.7159, Bias2: 0.9605, Total error: 1.6764\n",
      "Variance: 0.5295, Bias2: 1.2807, Total error: 1.8102\n",
      "Variance: 0.8720, Bias2: 1.6932, Total error: 2.5652\n",
      "Variance: 0.4601, Bias2: 0.7551, Total error: 1.2152\n",
      "Variance: 0.7946, Bias2: 1.7509, Total error: 2.5455\n",
      "Variance: 0.7886, Bias2: 1.1658, Total error: 1.9544\n",
      "Variance: 0.3192, Bias2: 0.7032, Total error: 1.0224\n",
      "Variance: 0.5002, Bias2: 1.4701, Total error: 1.9704\n",
      "Variance: 0.8356, Bias2: 1.4281, Total error: 2.2637\n",
      "Variance: 0.4273, Bias2: 0.8570, Total error: 1.2843\n",
      "model number: 6.000000 total error :1.022392\n",
      "Best Model Values: Variance: 0.6559, Bias2: 1.3245, Total error: 1.9805\n",
      "Degree: 1 MSE: 0.6606481791336333\n",
      "Variance: 2.2661, Bias2: 0.9596, Total error: 3.2257\n",
      "Variance: 0.6689, Bias2: 1.2433, Total error: 1.9122\n",
      "Variance: 0.7389, Bias2: 1.6719, Total error: 2.4108\n",
      "Variance: 0.4416, Bias2: 0.7586, Total error: 1.2003\n",
      "Variance: 0.7573, Bias2: 1.7491, Total error: 2.5064\n",
      "Variance: 0.8264, Bias2: 1.1704, Total error: 1.9969\n",
      "Variance: 0.4521, Bias2: 0.6752, Total error: 1.1273\n",
      "Variance: 0.5263, Bias2: 1.4629, Total error: 1.9892\n",
      "Variance: 0.7195, Bias2: 1.4036, Total error: 2.1231\n",
      "Variance: 0.5898, Bias2: 0.8340, Total error: 1.4238\n",
      "model number: 6.000000 total error :1.127293\n",
      "Best Model Values: Variance: 0.7092, Bias2: 1.3242, Total error: 2.0334\n",
      "Degree: 2 MSE: 0.6253809419284145\n",
      "Variance: 25.0727, Bias2: 0.9454, Total error: 26.0182\n",
      "Variance: 0.6731, Bias2: 1.2442, Total error: 1.9173\n",
      "Variance: 0.7162, Bias2: 1.6498, Total error: 2.3660\n",
      "Variance: 0.4483, Bias2: 0.7573, Total error: 1.2055\n",
      "Variance: 0.8145, Bias2: 1.7429, Total error: 2.5575\n",
      "Variance: 1.0222, Bias2: 1.1795, Total error: 2.2017\n",
      "Variance: 0.4487, Bias2: 0.6679, Total error: 1.1166\n",
      "Variance: 0.5575, Bias2: 1.4701, Total error: 2.0276\n",
      "Variance: 0.8072, Bias2: 1.3860, Total error: 2.1932\n",
      "Variance: 0.5494, Bias2: 0.8287, Total error: 1.3781\n",
      "model number: 6.000000 total error :1.116568\n",
      "Best Model Values: Variance: 0.7291, Bias2: 1.3241, Total error: 2.0532\n",
      "Degree: 3 MSE: 0.6088136722970662\n",
      "Variance: 963.4464, Bias2: 1.4386, Total error: 964.8850\n",
      "Variance: 0.6859, Bias2: 1.2419, Total error: 1.9278\n",
      "Variance: 0.7113, Bias2: 1.6341, Total error: 2.3454\n",
      "Variance: 0.4668, Bias2: 0.7571, Total error: 1.2239\n",
      "Variance: 0.8315, Bias2: 1.7403, Total error: 2.5718\n",
      "Variance: 1.0858, Bias2: 1.1823, Total error: 2.2681\n",
      "Variance: 0.4650, Bias2: 0.6472, Total error: 1.1123\n",
      "Variance: 0.5851, Bias2: 1.4718, Total error: 2.0570\n",
      "Variance: 0.8190, Bias2: 1.3749, Total error: 2.1939\n",
      "Variance: 0.5726, Bias2: 0.8258, Total error: 1.3985\n",
      "model number: 6.000000 total error :1.112269\n",
      "Best Model Values: Variance: 0.7364, Bias2: 1.3240, Total error: 2.0603\n",
      "Degree: 4 MSE: 0.5959499299346218\n",
      "Variance: 252972.2232, Bias2: 146.9534, Total error: 253119.1766\n",
      "Variance: 0.7376, Bias2: 1.2467, Total error: 1.9842\n",
      "Variance: 0.7293, Bias2: 1.6297, Total error: 2.3590\n",
      "Variance: 0.4660, Bias2: 0.7572, Total error: 1.2232\n",
      "Variance: 0.8602, Bias2: 1.7396, Total error: 2.5998\n",
      "Variance: 1.0835, Bias2: 1.1798, Total error: 2.2634\n",
      "Variance: 1.8967, Bias2: 0.6708, Total error: 2.5675\n",
      "Variance: 0.5887, Bias2: 1.4715, Total error: 2.0602\n",
      "Variance: 0.8222, Bias2: 1.3741, Total error: 2.1963\n",
      "Variance: 0.6824, Bias2: 0.8270, Total error: 1.5094\n",
      "model number: 3.000000 total error :1.223199\n",
      "Best Model Values: Variance: 0.7571, Bias2: 1.3232, Total error: 2.0803\n",
      "Degree: 5 MSE: 0.5895749538458621\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "def polynomial_regression(degree, X, y, folds, test_size=0.25, random_state=None):\n",
    "    # Define number of folds for cross-validation\n",
    "    kf = KFold(folds)\n",
    "\n",
    "    # Initialize lists to store results for variance, bias2s, total_error, and models\n",
    "    # TODO\n",
    "    variance_list = []\n",
    "    bias2s_list = []\n",
    "    total_error = []\n",
    "    models = []\n",
    "\n",
    "    # Set the polynomial degree of the model\n",
    "    poly_features = PolynomialFeatures(degree)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in kf.split(X_poly):\n",
    "        # Split data into training and testing sets for this fold\n",
    "        X_train, X_test = X_poly[train_index], X_poly[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit polynomial regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Calculate variance and Bias^2 for this fold\n",
    "        variance = np.mean(((y_pred)-np.mean(y_pred))**2)\n",
    "        bias2 = np.mean(((np.mean(y_pred)- y_test)) ** 2)\n",
    "\n",
    "        # Append results to lists\n",
    "        variance_list.append(variance)\n",
    "        bias2s_list.append(bias2)\n",
    "        total_error.append((bias2+variance))\n",
    "        models.append(model)\n",
    "\n",
    "        # Print results  including variance, Bias^2 and the total_error for this fold.\n",
    "        print(\"Variance: {:.4f}, Bias2: {:.4f}, Total error: {:.4f}\".format(variance, bias2, bias2 + variance))\n",
    "\n",
    "    # print the total_error of the best model\n",
    "    min_error_index = np.argmin(total_error)\n",
    "    best_model = models[min_error_index]\n",
    "    print(\"model number: {:1f} total error :{:4f}\".format(min_error_index, total_error[min_error_index]))\n",
    "\n",
    "    # Testing the final model on the test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=42)\n",
    "    # Obtain the predictions on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    variance = np.mean(((y_pred)-np.mean(y_pred))**2)\n",
    "    bias2 = np.mean(((np.mean(y_pred)- y_test)) ** 2)\n",
    "    print(\"Best Model Values: Variance: {:.4f}, Bias2: {:.4f}, Total error: {:.4f}\".format(variance, bias2, bias2 + variance))\n",
    "    # Calculate the mean squared error (MSE) on the test data\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    #also find error\n",
    "    \n",
    "    return mse , best_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage: load California Housing Dataset and select the first, third, and forth attributes as input features in X\n",
    "# TODO\n",
    "housing_data = fetch_california_housing()\n",
    "X = housing_data.data[:, [0, 2, 3]] #: means all rows, [0,2,3] means these specific columns\n",
    "y = housing_data.target\n",
    "\n",
    "# Set the target variable -- what goes into y, ie what we have to predict.\n",
    "\n",
    "degrees = range(1, 6)  # Try polynomial degrees from 1 to 5\n",
    "\n",
    "for degree in degrees:\n",
    "    mse, best_model = polynomial_regression(degree, X, y, folds=10, random_state=42)\n",
    "    print(\"Degree:\", degree, \"MSE:\", mse)\n",
    "# Try degrees from 1 to 5 and in a loop, report mse of the best model trained using k-fold cross validation and print(\"Degree:\", degree, \"MSE:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6024d-1023-4c9b-abae-28b2528b3306",
   "metadata": {
    "id": "8ed6024d-1023-4c9b-abae-28b2528b3306"
   },
   "source": [
    "#ANALYSIS\n",
    "**Analyse the results of model performance according to different degrees of polynomial and the number of folds used. You can manipulate the code and share your analysis in terms of the performance of the model (mse and total error),\n",
    "such as for instnace which degree of the model complexity (in relation to the polynomial order) would give a better model? Feel free to include other analysis about the generated models in relation to their performance results.\n",
    "You can event plot the results to support your analysis.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cTdTVOvC6dx",
   "metadata": {
    "id": "9cTdTVOvC6dx"
   },
   "source": [
    "One interesting pattern I noticed with the default k-fold validation function settings was that the first split in each degree would have significantly higher variance and bias compared to the subsequent splits, exponentially so for the higher degrees. However, when I set the shuffle parameter to true, that problem went away, indicating that the first segment of data had a significant amount of outliers. Although shuffling the folds helped with the first fold, it resulted in higher error throughout the other folds because the outliers were now distributed more evenly. Thus, I set shuffle to false again to prevent the outliers from affecting the other folds (although EDA would typically prune those values). Analyzing these results, the best model (purely by the metric of MSE) was the polynomial model of degree 3 [0.667], followed closely by the model of degree 1 [0.614]. However, looking at the metric of total error (variance + bias squared) calculated from the test split results, the degree 1 polynomial model slightly edged out the degree 3 polynomial model for the top spot, with a total error of 2.0012 compared to 2.0615. These results indicate that the relationship between the input data and target values is on the simpler side, and adding more degrees would lead to overfitting. Although the total error is lower for the one-degree model, the bias squared is the same for both, meaning the average model prediction is about the same distance from the ground truth. The difference between the two total errors thus solely originates from the variance, which only provides information on model complexity and the variability of its predictions, which naturally increases as the model becomes more complicated. Therefore, since the difference in total error can solely be attributed to the higher complexity of the three-degree model, with little bearing on the actual reliability of the model, the lower MSE of the three-degree model in comparison to the one-degree model leads me to believe that it is the best polynomial model for this dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
