{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61feddcd",
   "metadata": {},
   "source": [
    "#### Homework2\n",
    "Please explain clearly and include your entire computational work when needed. Should you include any code, please make sure to provide additional comments to explain your solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe85ed4",
   "metadata": {},
   "source": [
    "Q1 (8 points) Answer the following questions clearly. \n",
    "-\n",
    "- (4 points) Compare the cost functions in Ridge and Lasso Regression and indicate the regularization parameter.\n",
    "  --\n",
    "    Ridge Regression\n",
    "  $$\n",
    "    \\sum_{i=1}^{M}= \\sum_{i=1}^{M}\\left(y_i - \\sum_{j=0}^{N}w_j \\times x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "  $$\n",
    "   The regularization term for ridge regression is the sum of the squares of the coefficients, $\\sum_{j=1}^{n} w_j^2$, multiplied by the regularization parameter $\\lambda$. This adds a penalty equal to the square of the magnitude of coefficients (times lambda).\n",
    "  \n",
    "    Lasso Regression\n",
    "  $$\n",
    "   \\sum_{i=1}^{M}= \\sum_{i=1}^{M}\\left(y_i - \\sum_{j=0}^{N}w_j \\times x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "  $$\n",
    "   The regularization term for ridge regression is the sum of the absolute values of the coefficients, $\\sum_{j=1}^{n} |w_j|$, multiplied by the regularization parameter $\\lambda$. This adds a penalty equal to the absolute values of the magnitude of coefficients (times lambda).\n",
    "\n",
    "- (4 points) Explain which weights are more penalized in Ridge Regression and why (discuss your answer in the context of constraint satisfaction and take into account the constraint on Ridge Regression coefficients).\n",
    "    --\n",
    "    Large weights are much more penalized in Ridge Regression, as paramters must meet the following constraint on ridge regression scoefficients: for some $c>0$, $\\sum_{j=1}^{n} w_j^2 < c$. Since square a number massively increases its magnitude, large weights are penalized and, overall, weights will be forced to be smaller to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac371d",
   "metadata": {},
   "source": [
    "Q2 (12 points) In the context of training a linear regression model using Maximum-Likelihood-Estimation, answer the following questions:\n",
    "-\n",
    "(4 points) Indicate all assumptions discussed in the lecture under the MLE principle about the data, residual error, and the type of the probability density function used in the Likelihood function.\n",
    "--\n",
    "- Residual error is assumed to be normally distributed according to $\\epsilon \\sim N(0,\\sigma^2)$ where $/sigma$ is a constant and $\\epsilon$ is independent across observations (and thus variable $y$ is also independent across observations).\n",
    "- Also assume $x_0 = 1$ (because it is the bias term).\n",
    "- $x_1, x_2, ... , ğ‘¥_n$ are the independent variables, and $y$ in the dependent variable.\n",
    "- Y is normally distributed is equal to $N(g(x), \\sigma^2)$ where g(x) is mean and $\\sigma^2$ is variance.\n",
    "- The type of PDF used in the likelihood function is that of a normal distribution, i.e.:\n",
    "  $$\n",
    "    pdf(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\times e^{\\frac{\\left(y-\\mu\\right)^2}{2\\sigma^2}}\n",
    "  $$\n",
    "\n",
    "\n",
    "(4 points) Indicate the Likelihood function mathematically with respect to the assumptions made under MLE principle, and describe each term/parameters used in the likelihood function.\n",
    "--\n",
    "$$\n",
    "L(w_0, w_1, ..., w_n, \\sigma^2 | x,y) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\times e^{\\frac{\\left(y-\\mu\\right)^2}{2\\sigma^2}}\n",
    "$$\n",
    "- Terms/parameters in likelihood function: $\\theta$ is the set of all weights $w_0, w_1, ..., w_n$ AND the variance $\\sigma^2$.\n",
    "- The variance, $\\sigma^2$, is the variance of the errors/residuals in the linear regression model.\n",
    "- y is the normally distributed target/output variable, and x is the normally distributed input/feature variable.\n",
    "- The likelihood function represents the joint probability of all independent observed data $x_0, x_1, .., x_n$\n",
    "- It's the product of the likelihood of observing the data, given the model parameters($w_0, w_1, ..., w_n$)\n",
    "\n",
    "\n",
    "(4 points) Explain how the concept of maximizing the likelihood of observing data under model parameters is convertible to minimizing the NLL? Discuss in terms of the mathematical notation and the shape of the function. \n",
    "--\n",
    "These two concepts are mathematically equivalent. Why? Because all the NLL does is manipulate the form of the Likelihood function to make it easier to convert. \n",
    "Addition is easier to compute than multiplication, and thus, by taking the log of the Likelihood function, we can take advantage of the log property (log(AB) = log(A) + log(B)) to get the Log of Likelihood function in terms of addition rather than multiplication.\n",
    "\n",
    "So we go from: \n",
    "$$\n",
    "L(\\theta | x_1, x_2, ..., x_n) = \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\times e^{\\frac{\\left(y-\\mu\\right)^2}{2\\sigma^2}}\n",
    "$$\n",
    "to...\n",
    "$$\n",
    "log (L(w_0, w_1, ..., w_n, \\sigma^2 | x,y)) = \\sum_{i=1}^nlog((\\frac{1}{\\sqrt{2\\pi\\sigma^2}})e^{\\frac{\\left(y-\\mu\\right)^2}{2\\sigma^2}})\n",
    "$$\n",
    "to...\n",
    "$$\n",
    "log (L(w_0, w_1, ..., w_n, \\sigma^2 | x,y)) = \\sum_{i=1}^nlog(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}) + log(e^{\\frac{\\left(y-\\mu\\right)^2}{2\\sigma^2}})\n",
    "$$\n",
    "to...\n",
    "$$\n",
    "log (L(\\theta | x_1, x_2, ..., x_n)) = \\sum_{i=1}^n \\frac{n}{2}log(2\\pi\\sigma^2) +\\frac{1}{2\\sigma^2}\\sum^n_{i=1} \\left(y-wx_i\\right)^2\n",
    "$$\n",
    "\n",
    "and then negating this function, we get the function in a similar form to the OLS, where we're looking for the minimum in the \"bowl shape\", where the slope of the NLL is zero. This is the same as finding the point where the slope of the likelihood function is 0 -- i.e. where the NLL is minimum is the same concept as where the Likelihood function is maximum.\n",
    "$$\n",
    "log (L(\\theta | x_1, x_2, ..., x_n)) = - \\sum_{i=1}^n \\frac{n}{2}log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum^n_{i=1} \\left(y-wx_i\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d067c09",
   "metadata": {},
   "source": [
    "Q3 (10 points) Use the sklearn Breast_cancer dataset and use min-max scalar to transform the input attributes. Next, develop two classifiers using logistic regression, and perceptron learning. Train on the training data (75% of the entire data) and compare the performance of the models by reporting accuracy \"accuracy = accuracy_score(y_test, y_pred). Which model performs better? Provide your coding for the developed models and document your code. Failing proper documentation leads to losing points. Necessary library functions are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7efb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9790\n",
      "Perceptron Accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "x, y = data.data, data.target #set the feature and target values\n",
    "\n",
    "# Split dataset: 25% testing, 75% training. Set random state to 42 for consistency\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize minmax scaler, then scale both the training and testing data/features to fit in [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "#initialize logreg instance with randomstate 42 for consistency\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(x_train, y_train) #fit the model to the training data w/ logistic regression\n",
    "lr_y_pred = log_reg.predict(x_test) #perform prediction on test data\n",
    "lr_accuracy = accuracy_score(y_test, lr_y_pred) #find the accuracy score: (ytest - ypred)^2/n \n",
    "\n",
    "perceptron = Perceptron(random_state=42) #initialize instance of perceptron model w/ randomstate 42 for consistency.\n",
    "perceptron.fit(x_train, y_train) #fit perceptron model to training data w/ perceptron model\n",
    "p_y_pred = perceptron.predict(x_test) #feed x_test into trained model for ypred\n",
    "p_accuracy = accuracy_score(y_test, p_y_pred) #find the accuracy score: (ytest - ypred)^2/n \n",
    "\n",
    "# Output accuracy results\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"Perceptron Accuracy: {p_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1151ee6-145e-47b3-bb91-0e926851e887",
   "metadata": {},
   "source": [
    "The perceptron model performs better, as its accuracy is 0.986, higher than that of logistic regression (0.979). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac737a",
   "metadata": {},
   "source": [
    "Q4 (6 points) Compare and contrast Newton's method and gradient descent as optimization algorithms for finding the minimum of a function. Provide insights into their convergence properties, computational complexities, and practical considerations. Discuss situations where Newton's method should not be used. \n",
    "--\n",
    "\n",
    "[https://www.geeksforgeeks.org/newtons-method-in-machine-learning/]\n",
    "\n",
    "\n",
    "Newton's method and gradient descent are both iterative optimization algorithms, but differ in multiple ways.\n",
    "\n",
    "### Learning Rate:\n",
    "Newton's method uses the gradient and the second derivative of the function to find the minimum. It can converge very quickly once it's close to the solution, given that the function is well-behaved (smooth, convex, twice-differentiable). However, it might never converge if the function is not well-behaved.\n",
    "\n",
    "Meanwhile, gradient-descent only requires the first derivative, or the gradient. It converges linearly (affected by learning rate), requiring more iterations to converge than Newton's method.\n",
    "\n",
    "### Computational complexity:\n",
    "Newton's method is more computationally complicated to compute the second derivative/inverse of Hessian matrix, taking up to $O(n^3)$ to solve for dense linear systems-- making it too expensive for high-dimensional problems.\n",
    "\n",
    "Gradient descent is more computationally efficient, because it only requires the first order derivative, with each step being computationally cheaper than that of Newton's method. However, this may be balanced out by the higher number of interations required to reach solution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08c1f2",
   "metadata": {},
   "source": [
    "Q5 (6 points) Mathematically explain how a perceptron learning model is trained. Discuss in terms of the gradient of the error function used in Perceptron Learning algorithm. \n",
    "--\n",
    "\n",
    "Like normal gradient descent, we want to find the derivative of the error function with respect to the weights to iteratively improve the model weights. We call this *backpropagation* in the context of perceptron learning models.\n",
    "\n",
    "$$\n",
    "w_{new} = w_{current} - \\alpha \\Delta w\n",
    "$$\n",
    "and we define $\\delta w $ as the following:\n",
    "\n",
    "$$\n",
    "\\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{jk}}\n",
    "$$\n",
    "\n",
    "The gradient of the error function in Perceptron learning involves the *chain rule*, because there are many intermediary functions between the error function in the weights -- like the activation function, weights, another activation function... etc.\n",
    "\n",
    "for example, if we are looking at the weights of the output layer, our $\\delta w$ might look like:\n",
    "$$\n",
    "\\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{jk}} = \\frac{\\partial E_n(w)}{\\partial a_{k}} \\times \\frac{\\partial a_k}{\\partial z_{k}} \\times \\frac{\\partial z_k}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "all this to get the derivative of the error function in terms of the weights of that layer, which we then use per usual in the weight update equation as the $\\Delta w$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f0128",
   "metadata": {},
   "source": [
    "Q6 (4 points) Compare a Perceptron Learning algorithm with \"Binary Step function\" used as activation function, with a linear regression function in the context of binary classification. \n",
    "--\n",
    "\n",
    "First, let's get definitions out of the way.\n",
    "\n",
    "\"Binary Step\" function is defined as 0 when the input is negative,and 1 when the input is 0 or positive.\n",
    "\"Linear regression\" function essentially doesn't include an activation function, directly outputting the values calculated from the weights and biases.\n",
    "\n",
    "Because the binary step function is non-linear, it's only a \"hard classifier\"-- meaning that it only provides class labels without probabilistic context.\n",
    "Meanwhile, Lienar Regression outputs continuous values, which a threshold is then applied to in order to classify the value into a binary category.\n",
    "\n",
    "Binary classification learns decision boundary by adjusting weights for misclassified points. It updates weights for misclassified points to reduce error, such that the data points are correctly clasified (given that the data is linearly separable). This learned boundary is a hard cutoff, and we're essentially moving the decision boundary with each update. If it's not linearly separable, it will not converge.\n",
    "\n",
    "Linear regression also finds a linear boundary between two classes, but via minimizing the mean square error, making it more sensitive to outliers and exact value of each data point. And since linear regression doesn't enforce a strict boundary, it's not exactly optimal. It can handle both linear and non-linearly separable data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5219f68",
   "metadata": {},
   "source": [
    "Q7 (10 points) Answer the following questions: \n",
    "-\n",
    "(6 points) Discuss the vanishing gradient problem in the context of training deep neural networks and identify activation functions that are particularly susceptible to this phenomenon. \n",
    "--\n",
    "\n",
    "Vanishing Gradient Problem is a common problem when training neural networks that slows or even halts the learning process. This problem happens when the gradient of the loss function with respect to the weights becomes very small, especially in the lower/initial levels of the network.Because smaller gradients don't contribute much to weight updates, the network's weights won't update effectively. \n",
    "\n",
    "\n",
    "For large negative input values, softplus can cause vanishing gradient problem. \n",
    "For sigmoid, large positive or negative values can cause gradients approaching zero.\n",
    "Tanh has a similar problem, although it's slightly less pronounced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "280f34e4-fc00-43e4-b0a6-34441135eb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-import necessary libraries after environment reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range of x values\n",
    "x = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "sigmoid_values = sigmoid(x)\n",
    "tanh_values = tanh(x)\n",
    "softplus_values = softplus(x)\n",
    "\n",
    "# Plot each function\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Sigmoid plot\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(x, sigmoid_values, label=\"Sigmoid\", color=\"blue\")\n",
    "plt.title(\"Sigmoid Activation Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"sigmoid(x)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Tanh plot\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(x, tanh_values, label=\"Tanh\", color=\"green\")\n",
    "plt.title(\"Tanh Activation Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"tanh(x)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Softplus plot\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(x, softplus_values, label=\"Softplus\", color=\"red\")\n",
    "plt.title(\"Softplus Activation Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"softplus(x)\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Display plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1bcf1-93b0-4cdd-8e48-3f5fd6110b5f",
   "metadata": {},
   "source": [
    "  \n",
    "(4 points) Explain why these activation functions lead to vanishing gradients during backpropagation (hint: discuss in terms of the shape of the activation function). \n",
    "--\n",
    "As seen from the plots above, all these functions have \"saturation points\" where any value beyond that point has a rate of change close to 0.\n",
    "\n",
    "For sigmoid and tanh, this occurs as we approach negative infinity AND infinity.\n",
    "For the softplus function, this only occurs for negative values, especially for those with biger magnitudes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b950753",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Q8 (10 points) List all hyperparameters discussed in the class related to Artificial and Deep Neural Networks and explain the role/impact of each hyperparameter. Which technique(s) can be used to perform hyperparameter tuning? Explain how the technique(s) work. \n",
    "--\n",
    "**Activation function**\n",
    "Role: Determines the type of transformation applied to the input of each neuron in the network. Common activation functions include sigmoid, tanh, ReLU, Leaky ReLU, etc.... Each function has different characteristics:\n",
    "Sigmoid and tanh work for small networks but can cause vanishing gradients in deep networks.\n",
    "ReLU is also commonly used in deep learning due to faster convergence and non-saturating behavior.\n",
    "The choice of activation function affects the model's ability to capture complex patterns. ReLU, for instance, improved training speed but can suffer from the \"dying ReLU problem\", where some neurons output zero for all inputs.\n",
    "\n",
    "\n",
    "**Learning rate value**\n",
    "Determines the size of the steps taken in the direction of the gradient during weight updates. Typical values range from 0.001 to 0.1, depending on the problem. A high learning rate can lead to faster convergence but may overshoot the optimal solution, while a low rate may lead to slow training or getting stuck in local minima.\n",
    "\n",
    "\n",
    "**Number of epochs** \n",
    "(1 Epoch means when the whole training data is passed forward and backward through the neural\n",
    "network only once) Refers to the total number of times the training data passes through the entire network. More epochs allow the model to learn better but increase the risk of overfitting if set too high. If too low, the model may underfit.\n",
    "\n",
    "**Batch size**\n",
    " Specifies the number of samples processed before updating the modelâ€™s parameters. Typical values are powers of two (e.g., 32, 64, 128).\n",
    " Larger batch sizes can speed up training and lead to smoother gradient estimates, while smaller batch sizes provide more generalization but increase training time and variance.\n",
    "\n",
    "\n",
    "**Number of hidden layers**\n",
    " The number of hidden layers determines the depth of the neural network.\n",
    " More hidden layers allow the network to learn more complex representations but increase computational cost. Deep networks are better for complex problems, though they are more prone to overfitting.\n",
    "\n",
    "\n",
    "**Number of neurons at each hidden layer**\n",
    " Defines the width of each hidden layer, impacting the model's capacity.\n",
    "More neurons provide greater capacity to learn features but increase the risk of overfitting. If too few, the model may underfit.\n",
    "\n",
    "**Momentum term value**\n",
    "Helps accelerate gradient descent by adding a fraction of the previous gradient update to the current update. Common values range from 0.5 to 0.9. Increases convergence speed and helps the model escape shallow minima. A higher momentum can cause oscillations, while too low may slow down training.\n",
    "\n",
    "\n",
    "**Hidden layer activation function**\n",
    "Controls the non-linearity applied in hidden layers, similar to the activation function discussed above. However, the hidden layer activation function is often specifically optimized for deeper layers (e.g., ReLU).\n",
    "The activation function choice in hidden layers directly affects the network's ability to learn complex patterns.\n",
    "\n",
    "**Regularization Parameter value**\n",
    "Regularization methods like L2 (weight decay), L1 regularization, or dropout require a parameter to control the extent of regularization. A high regularization parameter reduces overfitting by penalizing large weights or randomly dropping nodes. If set too high, it can cause underfitting.\n",
    "\n",
    "Hyperparameter tuning techniques\n",
    "---\n",
    "\n",
    " **Manual Search** Run controlled experiemnts, where each experiment is a combination of hyperparameter values. Keep all but one parameter active, then analyze the effect and make your decision on which hyperparameter to change next, repeatng as necessary. Howeever, each experiment means training the model end-to-end, which is a very slow, especially for deep learning algorithms like CNNs.\n",
    "\n",
    " **Grid Search** systematically explores a defined grid of hyperparameters. The performance of the model on each combination of hyperparameters on a validation set is then evaluated. Hyperparameters resulting in teh best performance on the validation set are then chosen as the optimal hyperparameters. Model is trained on the training set w/ a specific set of hyperparameers, and the performance fo the model is evaluated on the validation set. This process is repeated for each combination of hyperparameters in the grid, and the best performing combination is selected. It's an automated \"trial-and-error\" process to selecte hyperparameters for the most accurate model. \n",
    "\n",
    " **Bayesian Optimization** builds a probabilistic model of the function that maps hyperparameters to model performance and iteratively selects the most promising hyperparameters based on this model. More efficient than grid search; ideal for high-dimensional spaces and complex models. Complex to implement and computationally intensive.\n",
    "\n",
    "\n",
    "**Cross-Validation Grid Search** is an exhaustive search method where all possible combinations of specified hyperparameter values are evaluated using cross-validation. Each combination of hyperparameters is validated using a cross-validation approach, typically k-fold cross-validation, which splits the dataset into k subsets, training on k-1 of them and validating on the remaining one, rotating each subset as the validation set once. Computationally expensive. \n",
    "\n",
    " **Random Search** randomly samples hyperparameter combinations instead of exhaustively searching all possible combinations. It defines a search space and randomly selects values for each hyperparameter within that space.Efficient for high-dimensional hyperparameter spaces since it doesnâ€™t evaluate every combination. No guarantee of finding the absolute best hyperparameters.\n",
    "\n",
    " \n",
    " **Bayesian Optimization** uses a probabilistic model to map hyperparameters to a performance measure based on past evaluations of the model. It iteratively updates its knowledge of hyperparameters and uses this model to decide which set of hyperparameters to evaluate next. Converges on a good solution faster than grid or random search, but complicated to implment.\n",
    "\n",
    "**Hyperband** is an adaptive resource allocation and early-stopping approach, based on principles of successive halving. Poorly performing configurations are halted early, making it faster than grid or random search since it doesnâ€™t evaluate all configurations fully.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57235d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Q9 (20 points) Given a dataset with input attributes x1 and x2, and output variable y, you are training a 3 layer neural network. Assume that activation function used in each layer is sigmoid. Mathematically describe one feed-forward pass followed by one backward-pass in terms of updating the weights of each layer in this neural network. \n",
    "--\n",
    "The number of hidden layer neurons is unknown in this formulation and there is only one output neuron. \n",
    "\n",
    "### Feed-Forward Pass\n",
    "$i$: $i^{th}$ neuron's output in the input layer\n",
    "$j$: $j^{th}$ neuron's output in the hidden layer\n",
    "$y$: output neuron's output\n",
    "\n",
    "$b_j$: biases in the hidden layer\n",
    "$b_k$: biases in the output layer\n",
    "\n",
    "$x_i$: data coming from the input layer\n",
    "$w_{ij}$: wegiths connecting input to hidden layer\n",
    "$w_{jk}$: wegiths connecting hidden to output layer\n",
    "\n",
    "$z_j$: biases in the hidden layer\n",
    "$z_k$: biases in the output layer\n",
    "\n",
    "**First Layer (Input -> Hidden 1):**\n",
    "   $$\n",
    "   z_{j} = w_{ij} \\cdot x_1 + b_j = \\sum_i w_{ij} x_i + b_j\n",
    "   $$\n",
    "   $$\n",
    "   a_1 = \\sigma(z_j)\n",
    "   $$\n",
    "\n",
    "**Second Layer (Hidden 1 -> Output):**\n",
    "   $$\n",
    "   z_{k} = w_{ij} \\cdot \\sigma(z_j)+ b_k\n",
    "   $$\n",
    "   $$\n",
    "   a_k = \\sigma {z_k} = \\hat{y} = \\sigma(z_k) = \\sigma (w_{jk} \\sigma (w_{ij}x_i + b_j) + b_k)\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "### Backward Pass (Using Gradient Descent)\n",
    "\n",
    "Define the loss function $ L = \\frac{1}{2} \\sum_k(\\hat{y} - y)^2 $. Then, calculate the gradients with respect to each layer's weights and biases to update them.\n",
    "\n",
    "1. **Output Layer Gradients:**\n",
    "  $$\n",
    "   \\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{jk}} = \\frac{\\partial E_n(w)}{\\partial a_{k}} \\times \\frac{\\partial a_k}{\\partial z_k} \\times \\frac{\\partial z_k}{\\partial w_{jk}}\n",
    "   $$\n",
    "Using the chain rule:\n",
    "   $$\n",
    "   \\frac{\\partial E_n(w)}{\\partial w_{jk}} = (y - \\hat{y})\\times \\frac{\\partial a_k}{\\partial z_k} \\times \\frac{\\partial z_k}{\\partial w_{jk}}  = (y - \\hat{y})\\times (\\sigma(z_k)(1-\\sigma (z_k)) \\times \\frac{\\partial z_k}{\\partial w_{jk}}\n",
    "   $$\n",
    "    remember...\n",
    "   $\\hat{y} = \\sigma(z_k)$, $z_k = w_{jk} \\sigma z_j + b_k$, and $z_j = w_{ij}x_i + b_j$\n",
    "\n",
    "   and derivative of sigmoid function:\n",
    "   $$\n",
    "   \\frac{\\partial \\sigma (x)}{\\partial x} = \\sigma(x) (1- \\sigma (x))\n",
    "   $$\n",
    "   $$\n",
    "   Sigmoid = \\sigma (x) = 1/(1+e^{-x})\n",
    "   $$\n",
    "3. **Hidden Layer Gradients:**\n",
    "    $$\n",
    "   \\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{ij}} = \\frac{\\partial E_n(w)}{\\partial a_{k}} \\times \\frac{\\partial a_k}{\\partial z_k} \\times \\frac{\\partial z_k}{\\partial a_j} \\times \\frac{\\partial a_j}{\\partial z_j} \\times \\frac{\\partial z_j}{\\partial w_{ij}}\n",
    "   $$\n",
    "Using the chain rule:\n",
    "   $$\n",
    "   \\frac{\\partial E_n(w)}{\\partial w_{ij}} = (y - \\hat{y})\\times \\frac{\\partial a_k}{\\partial z_k} \\times \\frac{\\partial z_k}{\\partial a_j}\\times \\frac{\\partial a_j}{\\partial z_j} \\times \\frac{\\partial z_j}{\\partial w_{ij}}  = (y - \\hat{y})\\times (\\sigma(z_k)(1-\\sigma (z_k)) \\times \\frac{\\partial z_k}{\\partial a_j}\\times \\frac{\\partial a_j}{\\partial z_j} \\times \\frac{\\partial z_j}{\\partial w_{ij}}\n",
    "   $$\n",
    "\n",
    "\n",
    "### Weight and Bias Updates\n",
    "\n",
    "Using a learning rate $ \\alpha $, update each layerâ€™s weights and biases:\n",
    "$$\n",
    "W_{jk} := W_{jk} - \\alpha \\frac{\\partial  E_n(w)}{\\partial W_{jk}}\n",
    "$$\n",
    "$$\n",
    "W_{ij} := W_{ij} - \\alpha \\frac{\\partial  E_n(w)}{\\partial W_{ij}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7de4e",
   "metadata": {},
   "source": [
    "Q10 (14 points) In this exercise, use the output information generated by this code to perform a comparative study of the performance (i.e., loss , Accuracy) of the neural networks models based on the hyperparameters used. Generate a table to report your analysis.\n",
    "Note: If you want to run this code and have trouble with imported libraries, try 'pip install keras==2.12.0'\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55c2190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.12.0 in /opt/anaconda3/envs/myenv/lib/python3.8/site-packages (2.12.0)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/jrvxkfl12xg2pqv0j17w8bsr0000gn/T/ipykernel_23640/4219747616.py:44: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 520us/step - loss: 2.3206 - accuracy: 0.1343\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 430us/step - loss: 2.0480 - accuracy: 0.2749\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 1.8404 - accuracy: 0.3820\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 1.6942 - accuracy: 0.4280\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.5364 - accuracy: 0.4920\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.3714 - accuracy: 0.5665\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 432us/step - loss: 1.2539 - accuracy: 0.6068\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 1.1114 - accuracy: 0.6340\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 422us/step - loss: 1.0363 - accuracy: 0.6701\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 436us/step - loss: 0.9323 - accuracy: 0.6980\n",
      "12/12 [==============================] - 0s 358us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 2.2841 - accuracy: 0.1677\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 2.0836 - accuracy: 0.2568\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 432us/step - loss: 1.8873 - accuracy: 0.3633\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 1.6945 - accuracy: 0.4287\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 1.5033 - accuracy: 0.4969\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.3528 - accuracy: 0.5616\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 424us/step - loss: 1.2063 - accuracy: 0.6068\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 1.1102 - accuracy: 0.6486\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 0.9914 - accuracy: 0.6729\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 0.9107 - accuracy: 0.6896\n",
      "12/12 [==============================] - 0s 356us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 976us/step - loss: 2.3006 - accuracy: 0.1280\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 536us/step - loss: 2.1182 - accuracy: 0.2156\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 423us/step - loss: 1.9551 - accuracy: 0.3088\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 1.7955 - accuracy: 0.3484\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 428us/step - loss: 1.6351 - accuracy: 0.4478\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.5141 - accuracy: 0.4889\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.2919 - accuracy: 0.5897\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 428us/step - loss: 1.1671 - accuracy: 0.6203\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 1.0773 - accuracy: 0.6370\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 433us/step - loss: 0.9939 - accuracy: 0.6592\n",
      "12/12 [==============================] - 0s 354us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 453us/step - loss: 2.2997 - accuracy: 0.1495\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 2.0621 - accuracy: 0.2615\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 426us/step - loss: 1.9216 - accuracy: 0.3435\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.7243 - accuracy: 0.4367\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 431us/step - loss: 1.5296 - accuracy: 0.5195\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 431us/step - loss: 1.3832 - accuracy: 0.5675\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 1.2120 - accuracy: 0.6224\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 432us/step - loss: 1.0737 - accuracy: 0.6634\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.9503 - accuracy: 0.6836\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 0.8941 - accuracy: 0.7191\n",
      "12/12 [==============================] - 0s 360us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 2.3087 - accuracy: 0.1502\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 2.0273 - accuracy: 0.3004\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.8508 - accuracy: 0.3644\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 1.6342 - accuracy: 0.4624\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 421us/step - loss: 1.4430 - accuracy: 0.5188\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 424us/step - loss: 1.2579 - accuracy: 0.6078\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 1.1663 - accuracy: 0.6161\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 424us/step - loss: 1.0184 - accuracy: 0.6565\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 0.9752 - accuracy: 0.6787\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 0.8327 - accuracy: 0.7344\n",
      "12/12 [==============================] - 0s 344us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 506us/step - loss: 2.2955 - accuracy: 0.1559\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 1.9207 - accuracy: 0.3716\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.5641 - accuracy: 0.5282\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 1.2745 - accuracy: 0.6145\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 1.0404 - accuracy: 0.6806\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.8990 - accuracy: 0.7356\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 0.7644 - accuracy: 0.7571\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.6759 - accuracy: 0.7919\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.5906 - accuracy: 0.8121\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.5603 - accuracy: 0.8024\n",
      "12/12 [==============================] - 0s 358us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 2.2934 - accuracy: 0.1475\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 2.0516 - accuracy: 0.2992\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 1.7757 - accuracy: 0.4579\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 1.4688 - accuracy: 0.5706\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 1.2393 - accuracy: 0.6305\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.9929 - accuracy: 0.7126\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 453us/step - loss: 0.8635 - accuracy: 0.7411\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 444us/step - loss: 0.7743 - accuracy: 0.7752\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 456us/step - loss: 0.6761 - accuracy: 0.7912\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.5957 - accuracy: 0.8253\n",
      "12/12 [==============================] - 0s 371us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 2.1730 - accuracy: 0.2038\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 432us/step - loss: 1.8582 - accuracy: 0.3950\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 1.5137 - accuracy: 0.5556\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 1.1610 - accuracy: 0.6662\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 433us/step - loss: 0.9531 - accuracy: 0.7288\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 428us/step - loss: 0.8025 - accuracy: 0.7615\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 433us/step - loss: 0.6777 - accuracy: 0.7949\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 444us/step - loss: 0.6240 - accuracy: 0.8102\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.5598 - accuracy: 0.8331\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 439us/step - loss: 0.4882 - accuracy: 0.8414\n",
      "12/12 [==============================] - 0s 345us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 519us/step - loss: 2.3069 - accuracy: 0.1634\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 1.9685 - accuracy: 0.3672\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 1.6387 - accuracy: 0.4937\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 1.3578 - accuracy: 0.5841\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 1.1527 - accuracy: 0.6412\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.9956 - accuracy: 0.6919\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.8517 - accuracy: 0.7427\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.7693 - accuracy: 0.7643\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.7004 - accuracy: 0.7782\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.5989 - accuracy: 0.8102\n",
      "12/12 [==============================] - 0s 356us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 432us/step - loss: 2.2225 - accuracy: 0.2128\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 442us/step - loss: 1.8659 - accuracy: 0.4242\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 1.5649 - accuracy: 0.5341\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 442us/step - loss: 1.2651 - accuracy: 0.6314\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 1.0237 - accuracy: 0.6961\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 453us/step - loss: 0.8868 - accuracy: 0.7302\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.7367 - accuracy: 0.7761\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 0.6833 - accuracy: 0.7900\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.6065 - accuracy: 0.8171\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 0.5571 - accuracy: 0.8241\n",
      "12/12 [==============================] - 0s 337us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 2.1656 - accuracy: 0.2554\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 1.4567 - accuracy: 0.5338\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 406us/step - loss: 0.9696 - accuracy: 0.6736\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.7261 - accuracy: 0.7502\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 398us/step - loss: 0.6013 - accuracy: 0.8051\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 0.4836 - accuracy: 0.8386\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 423us/step - loss: 0.4505 - accuracy: 0.8476\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.4022 - accuracy: 0.8685\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.3770 - accuracy: 0.8768\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 398us/step - loss: 0.3080 - accuracy: 0.9019\n",
      "12/12 [==============================] - 0s 332us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 2.0724 - accuracy: 0.2540\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 1.3110 - accuracy: 0.5595\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 0.8803 - accuracy: 0.7161\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 422us/step - loss: 0.6094 - accuracy: 0.7996\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 0.5251 - accuracy: 0.8344\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.4601 - accuracy: 0.8511\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 0.3879 - accuracy: 0.8740\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 0.3918 - accuracy: 0.8803\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 0.3583 - accuracy: 0.8942\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.3088 - accuracy: 0.9116\n",
      "12/12 [==============================] - 0s 385us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 2.0612 - accuracy: 0.2691\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 1.2701 - accuracy: 0.5855\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 406us/step - loss: 0.8841 - accuracy: 0.6954\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.6364 - accuracy: 0.7879\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 406us/step - loss: 0.5187 - accuracy: 0.8331\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 0.4555 - accuracy: 0.8498\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 0.3683 - accuracy: 0.8790\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 0.3415 - accuracy: 0.8908\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.3172 - accuracy: 0.8936\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 394us/step - loss: 0.2958 - accuracy: 0.9145\n",
      "12/12 [==============================] - 0s 336us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 2.1617 - accuracy: 0.2469\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 385us/step - loss: 1.4768 - accuracy: 0.5167\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 1.0053 - accuracy: 0.6530\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.7585 - accuracy: 0.7510\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 458us/step - loss: 0.6437 - accuracy: 0.7872\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 433us/step - loss: 0.5301 - accuracy: 0.8220\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 430us/step - loss: 0.4549 - accuracy: 0.8547\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 433us/step - loss: 0.3967 - accuracy: 0.8644\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 0.3277 - accuracy: 0.8915\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 437us/step - loss: 0.3171 - accuracy: 0.9019\n",
      "12/12 [==============================] - 0s 340us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 2.1825 - accuracy: 0.2218\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 431us/step - loss: 1.5038 - accuracy: 0.4965\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 439us/step - loss: 0.9581 - accuracy: 0.6905\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 430us/step - loss: 0.7139 - accuracy: 0.7782\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 434us/step - loss: 0.5571 - accuracy: 0.8268\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 429us/step - loss: 0.4881 - accuracy: 0.8526\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 444us/step - loss: 0.4130 - accuracy: 0.8651\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 0.3951 - accuracy: 0.8734\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 426us/step - loss: 0.3587 - accuracy: 0.8797\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 435us/step - loss: 0.3047 - accuracy: 0.8978\n",
      "12/12 [==============================] - 0s 339us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 489us/step - loss: 2.0312 - accuracy: 0.3180\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 1.0022 - accuracy: 0.6903\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.5762 - accuracy: 0.8093\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 0.3723 - accuracy: 0.8887\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 0.2771 - accuracy: 0.9088\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 0.2336 - accuracy: 0.9241\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 0.2120 - accuracy: 0.9304\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 0.1944 - accuracy: 0.9415\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.1482 - accuracy: 0.9485\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 485us/step - loss: 0.1562 - accuracy: 0.9499\n",
      "12/12 [==============================] - 0s 362us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 489us/step - loss: 1.9073 - accuracy: 0.3549\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 0.9834 - accuracy: 0.6862\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 0.5150 - accuracy: 0.8358\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.4006 - accuracy: 0.8650\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 0.3127 - accuracy: 0.8949\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.2343 - accuracy: 0.9235\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.2145 - accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.1912 - accuracy: 0.9332\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 0.1744 - accuracy: 0.9422\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 492us/step - loss: 0.1391 - accuracy: 0.9576\n",
      "12/12 [==============================] - 0s 352us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 484us/step - loss: 1.8063 - accuracy: 0.4033\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 501us/step - loss: 0.7563 - accuracy: 0.7775\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 490us/step - loss: 0.4618 - accuracy: 0.8463\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 484us/step - loss: 0.3504 - accuracy: 0.8873\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.2662 - accuracy: 0.9159\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 0.2304 - accuracy: 0.9214\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 487us/step - loss: 0.1898 - accuracy: 0.9416\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 487us/step - loss: 0.1624 - accuracy: 0.9506\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.1605 - accuracy: 0.9541\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 497us/step - loss: 0.1599 - accuracy: 0.9499\n",
      "12/12 [==============================] - 0s 347us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 509us/step - loss: 2.0503 - accuracy: 0.3220\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 1.0213 - accuracy: 0.6829\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.6304 - accuracy: 0.8081\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 484us/step - loss: 0.4503 - accuracy: 0.8658\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 489us/step - loss: 0.3191 - accuracy: 0.8943\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 516us/step - loss: 0.2736 - accuracy: 0.9089\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 488us/step - loss: 0.2278 - accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 0.1953 - accuracy: 0.9339\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 499us/step - loss: 0.1531 - accuracy: 0.9506\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.1694 - accuracy: 0.9492\n",
      "12/12 [==============================] - 0s 370us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 1.9957 - accuracy: 0.3303\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 436us/step - loss: 1.0374 - accuracy: 0.6912\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.5913 - accuracy: 0.8129\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 456us/step - loss: 0.4522 - accuracy: 0.8456\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.3020 - accuracy: 0.9047\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 0.2689 - accuracy: 0.9138\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.2258 - accuracy: 0.9166\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.2232 - accuracy: 0.9235\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 428us/step - loss: 0.1812 - accuracy: 0.9444\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.1454 - accuracy: 0.9527\n",
      "12/12 [==============================] - 0s 345us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 2.6257 - accuracy: 0.1086\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 2.3665 - accuracy: 0.1329\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 2.3124 - accuracy: 0.1510\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 2.2316 - accuracy: 0.1907\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 2.2200 - accuracy: 0.1865\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 2.1928 - accuracy: 0.1865\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.1083 - accuracy: 0.2401\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.1354 - accuracy: 0.2206\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 2.0499 - accuracy: 0.2617\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 2.0176 - accuracy: 0.2582\n",
      "12/12 [==============================] - 0s 350us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 2.6945 - accuracy: 0.1113\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 2.3973 - accuracy: 0.1225\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 2.3482 - accuracy: 0.1406\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 2.2608 - accuracy: 0.1614\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 2.2248 - accuracy: 0.1830\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 442us/step - loss: 2.2220 - accuracy: 0.1698\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 441us/step - loss: 2.1617 - accuracy: 0.1969\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 421us/step - loss: 2.1362 - accuracy: 0.2164\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 431us/step - loss: 2.0778 - accuracy: 0.2408\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 2.0839 - accuracy: 0.2387\n",
      "12/12 [==============================] - 0s 340us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 2.6868 - accuracy: 0.1043\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 2.3875 - accuracy: 0.1231\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 2.2907 - accuracy: 0.1516\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 430us/step - loss: 2.2467 - accuracy: 0.1662\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.1918 - accuracy: 0.1815\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 2.1856 - accuracy: 0.2072\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 2.1579 - accuracy: 0.1898\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.0858 - accuracy: 0.2281\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 437us/step - loss: 2.0667 - accuracy: 0.2357\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.9949 - accuracy: 0.2594\n",
      "12/12 [==============================] - 0s 336us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 2.6736 - accuracy: 0.1036\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 2.4088 - accuracy: 0.1293\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 2.3063 - accuracy: 0.1467\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 2.3017 - accuracy: 0.1474\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 2.2397 - accuracy: 0.1766\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 2.1949 - accuracy: 0.1871\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 2.1650 - accuracy: 0.2107\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 2.1390 - accuracy: 0.2225\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 2.1098 - accuracy: 0.2309\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 2.0253 - accuracy: 0.2552\n",
      "12/12 [==============================] - 0s 344us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 2.7177 - accuracy: 0.1057\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 429us/step - loss: 2.4113 - accuracy: 0.1259\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 2.3357 - accuracy: 0.1412\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.2786 - accuracy: 0.1648\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.2329 - accuracy: 0.1739\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 2.1922 - accuracy: 0.1961\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 2.1524 - accuracy: 0.2107\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 2.1100 - accuracy: 0.2364\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 2.0714 - accuracy: 0.2566\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 2.0182 - accuracy: 0.2775\n",
      "12/12 [==============================] - 0s 325us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 495us/step - loss: 2.5171 - accuracy: 0.1232\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 2.3192 - accuracy: 0.1594\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 2.1938 - accuracy: 0.2150\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 2.1250 - accuracy: 0.2352\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 2.0168 - accuracy: 0.2853\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 1.9236 - accuracy: 0.3285\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 1.8494 - accuracy: 0.3619\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 465us/step - loss: 1.7304 - accuracy: 0.3994\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 1.6528 - accuracy: 0.4454\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 1.5323 - accuracy: 0.4669\n",
      "12/12 [==============================] - 0s 346us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 2.6689 - accuracy: 0.1169\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 489us/step - loss: 2.3478 - accuracy: 0.1733\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 2.2081 - accuracy: 0.2025\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 2.0802 - accuracy: 0.2665\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 1.9891 - accuracy: 0.2944\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.8985 - accuracy: 0.3479\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 1.8098 - accuracy: 0.3911\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 1.6444 - accuracy: 0.4509\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 1.5957 - accuracy: 0.4537\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.4827 - accuracy: 0.4809\n",
      "12/12 [==============================] - 0s 363us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 497us/step - loss: 2.6132 - accuracy: 0.1092\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 2.3351 - accuracy: 0.1481\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 2.2240 - accuracy: 0.1919\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 2.1222 - accuracy: 0.2385\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 2.0409 - accuracy: 0.2740\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 466us/step - loss: 1.9248 - accuracy: 0.3268\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 1.8132 - accuracy: 0.3679\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 1.7414 - accuracy: 0.3880\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 514us/step - loss: 1.6431 - accuracy: 0.4207\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 496us/step - loss: 1.6037 - accuracy: 0.4444\n",
      "12/12 [==============================] - 0s 365us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 615us/step - loss: 2.6901 - accuracy: 0.0960\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 772us/step - loss: 2.3828 - accuracy: 0.1252\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 499us/step - loss: 2.2585 - accuracy: 0.1704\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 480us/step - loss: 2.1259 - accuracy: 0.2211\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 2.0673 - accuracy: 0.2698\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 1.9419 - accuracy: 0.3317\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.8612 - accuracy: 0.3449\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 1.8114 - accuracy: 0.3581\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 492us/step - loss: 1.6596 - accuracy: 0.4277\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 1.5613 - accuracy: 0.4451\n",
      "12/12 [==============================] - 0s 352us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 500us/step - loss: 2.6114 - accuracy: 0.1189\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 2.3031 - accuracy: 0.1808\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 2.2230 - accuracy: 0.1843\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 2.1436 - accuracy: 0.2246\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 2.0263 - accuracy: 0.2768\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 1.9789 - accuracy: 0.2872\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 1.8625 - accuracy: 0.3540\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 1.7558 - accuracy: 0.3665\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 487us/step - loss: 1.6522 - accuracy: 0.4444\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 485us/step - loss: 1.6012 - accuracy: 0.4339\n",
      "12/12 [==============================] - 0s 351us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 2.3719 - accuracy: 0.1371\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 424us/step - loss: 2.1455 - accuracy: 0.2053\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 1.9726 - accuracy: 0.3111\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.8531 - accuracy: 0.3577\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 421us/step - loss: 1.7177 - accuracy: 0.4210\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 429us/step - loss: 1.5950 - accuracy: 0.4558\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 1.4711 - accuracy: 0.4962\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 1.4176 - accuracy: 0.5052\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.3092 - accuracy: 0.5539\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 1.3163 - accuracy: 0.5637\n",
      "12/12 [==============================] - 0s 336us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 2.4416 - accuracy: 0.1434\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 2.1570 - accuracy: 0.2039\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.9646 - accuracy: 0.2860\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.7637 - accuracy: 0.3528\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 1.6763 - accuracy: 0.4106\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 1.5823 - accuracy: 0.4468\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 1.4277 - accuracy: 0.4781\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.3406 - accuracy: 0.5275\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 1.2728 - accuracy: 0.5449\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 422us/step - loss: 1.2226 - accuracy: 0.5637\n",
      "12/12 [==============================] - 0s 337us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 2.4100 - accuracy: 0.1460\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 2.0787 - accuracy: 0.2629\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.8785 - accuracy: 0.3338\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 1.7312 - accuracy: 0.3936\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 406us/step - loss: 1.5412 - accuracy: 0.4805\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 1.5016 - accuracy: 0.5028\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 1.3393 - accuracy: 0.5487\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 1.2528 - accuracy: 0.5737\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 1.2014 - accuracy: 0.5828\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 1.1169 - accuracy: 0.6307\n",
      "12/12 [==============================] - 0s 347us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 2.4974 - accuracy: 0.1280\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 2.2089 - accuracy: 0.2017\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 2.0465 - accuracy: 0.2517\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.8830 - accuracy: 0.3060\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 1.7226 - accuracy: 0.3915\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 1.6060 - accuracy: 0.4159\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 1.4897 - accuracy: 0.4708\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 1.4382 - accuracy: 0.4979\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.3223 - accuracy: 0.5494\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 1.3108 - accuracy: 0.5438\n",
      "12/12 [==============================] - 0s 315us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 443us/step - loss: 2.4153 - accuracy: 0.1530\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 2.0952 - accuracy: 0.2392\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 406us/step - loss: 1.9181 - accuracy: 0.2921\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 1.7373 - accuracy: 0.3734\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 1.5876 - accuracy: 0.4353\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 1.4853 - accuracy: 0.4896\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.4150 - accuracy: 0.5070\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.3740 - accuracy: 0.5097\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.2403 - accuracy: 0.5730\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 1.2235 - accuracy: 0.5821\n",
      "12/12 [==============================] - 0s 358us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 495us/step - loss: 2.3882 - accuracy: 0.1614\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 1.9231 - accuracy: 0.3069\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 1.5507 - accuracy: 0.4656\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 1.2670 - accuracy: 0.5678\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 488us/step - loss: 1.0866 - accuracy: 0.6242\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 503us/step - loss: 0.9781 - accuracy: 0.6882\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 506us/step - loss: 0.8841 - accuracy: 0.7001\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 501us/step - loss: 0.8188 - accuracy: 0.7279\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 505us/step - loss: 0.7009 - accuracy: 0.7773\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 520us/step - loss: 0.6871 - accuracy: 0.7780\n",
      "12/12 [==============================] - 0s 345us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 494us/step - loss: 2.3972 - accuracy: 0.1510\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 1.9602 - accuracy: 0.2937\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.5474 - accuracy: 0.4739\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.2725 - accuracy: 0.5762\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.1121 - accuracy: 0.6437\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.9400 - accuracy: 0.6931\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 0.8431 - accuracy: 0.7196\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.7701 - accuracy: 0.7523\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 0.7160 - accuracy: 0.7648\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.6784 - accuracy: 0.7940\n",
      "12/12 [==============================] - 0s 389us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 495us/step - loss: 2.4570 - accuracy: 0.1453\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 1.9500 - accuracy: 0.3067\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 1.5896 - accuracy: 0.4318\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 1.2822 - accuracy: 0.5682\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 1.1019 - accuracy: 0.6175\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 1.0353 - accuracy: 0.6613\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.8835 - accuracy: 0.7072\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.7613 - accuracy: 0.7608\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.6867 - accuracy: 0.7830\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 0.6619 - accuracy: 0.7928\n",
      "12/12 [==============================] - 0s 372us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 497us/step - loss: 2.4241 - accuracy: 0.1641\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 1.8896 - accuracy: 0.3456\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.4576 - accuracy: 0.4930\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 485us/step - loss: 1.2639 - accuracy: 0.5737\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 1.0942 - accuracy: 0.6252\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 0.9789 - accuracy: 0.6780\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.8763 - accuracy: 0.7204\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.7639 - accuracy: 0.7566\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 0.7263 - accuracy: 0.7747\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 0.6922 - accuracy: 0.7803\n",
      "12/12 [==============================] - 0s 334us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 493us/step - loss: 2.4030 - accuracy: 0.1592\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 459us/step - loss: 1.9028 - accuracy: 0.3122\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.5266 - accuracy: 0.4666\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 1.2884 - accuracy: 0.5549\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.1148 - accuracy: 0.6300\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.9854 - accuracy: 0.6933\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 0.8653 - accuracy: 0.7218\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 0.7676 - accuracy: 0.7483\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 0.7575 - accuracy: 0.7510\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.6444 - accuracy: 0.7955\n",
      "12/12 [==============================] - 0s 333us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 437us/step - loss: 2.1555 - accuracy: 0.2255\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 422us/step - loss: 1.6918 - accuracy: 0.4621\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 395us/step - loss: 1.4085 - accuracy: 0.5936\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 1.2273 - accuracy: 0.6527\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 1.0775 - accuracy: 0.7015\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 0.9568 - accuracy: 0.7481\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 0.8724 - accuracy: 0.7759\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 0.7992 - accuracy: 0.7919\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 0.7059 - accuracy: 0.8253\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 0.6812 - accuracy: 0.8239\n",
      "12/12 [==============================] - 0s 346us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 2.0955 - accuracy: 0.2477\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 396us/step - loss: 1.6166 - accuracy: 0.5101\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 1.3249 - accuracy: 0.6465\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 1.1412 - accuracy: 0.7084\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 0.9836 - accuracy: 0.7585\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 0.8903 - accuracy: 0.7717\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.7897 - accuracy: 0.8100\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.7134 - accuracy: 0.8239\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 0.6433 - accuracy: 0.8546\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.5945 - accuracy: 0.8643\n",
      "12/12 [==============================] - 0s 329us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 582us/step - loss: 2.2145 - accuracy: 0.1940\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 501us/step - loss: 1.7125 - accuracy: 0.4416\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 1.4010 - accuracy: 0.6029\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.2380 - accuracy: 0.6530\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 1.0901 - accuracy: 0.7121\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 0.9654 - accuracy: 0.7510\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 423us/step - loss: 0.8995 - accuracy: 0.7601\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 0.8096 - accuracy: 0.7865\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.7234 - accuracy: 0.8129\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 387us/step - loss: 0.6544 - accuracy: 0.8442\n",
      "12/12 [==============================] - 0s 362us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 602us/step - loss: 2.1196 - accuracy: 0.2483\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 805us/step - loss: 1.6388 - accuracy: 0.5049\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 702us/step - loss: 1.3725 - accuracy: 0.6154\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 729us/step - loss: 1.1831 - accuracy: 0.6634\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 1.0430 - accuracy: 0.7065\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 0.9378 - accuracy: 0.7378\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 521us/step - loss: 0.8425 - accuracy: 0.7684\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 506us/step - loss: 0.7551 - accuracy: 0.7935\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 441us/step - loss: 0.6973 - accuracy: 0.8108\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.6393 - accuracy: 0.8282\n",
      "12/12 [==============================] - 0s 409us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 593us/step - loss: 2.0607 - accuracy: 0.2663\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 672us/step - loss: 1.6269 - accuracy: 0.4777\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 1.3730 - accuracy: 0.6140\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 1.1828 - accuracy: 0.6871\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 385us/step - loss: 1.0593 - accuracy: 0.7107\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 434us/step - loss: 0.9316 - accuracy: 0.7580\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 423us/step - loss: 0.8459 - accuracy: 0.7712\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 0.7534 - accuracy: 0.8268\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 442us/step - loss: 0.6922 - accuracy: 0.8296\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 431us/step - loss: 0.6348 - accuracy: 0.8387\n",
      "12/12 [==============================] - 0s 373us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 1.8917 - accuracy: 0.3598\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 1.2083 - accuracy: 0.6660\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 458us/step - loss: 0.8990 - accuracy: 0.7669\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.7126 - accuracy: 0.8281\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.5871 - accuracy: 0.8615\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.5213 - accuracy: 0.8775\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.4581 - accuracy: 0.8935\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.4040 - accuracy: 0.9033\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.3695 - accuracy: 0.9074\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 0.3464 - accuracy: 0.9109\n",
      "12/12 [==============================] - 0s 353us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 1.9383 - accuracy: 0.3229\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 441us/step - loss: 1.2993 - accuracy: 0.6423\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 0.9882 - accuracy: 0.7620\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.7787 - accuracy: 0.8184\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.6470 - accuracy: 0.8490\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.5723 - accuracy: 0.8629\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.4899 - accuracy: 0.8873\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 453us/step - loss: 0.4408 - accuracy: 0.8914\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 447us/step - loss: 0.3969 - accuracy: 0.9040\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.3581 - accuracy: 0.9123\n",
      "12/12 [==============================] - 0s 363us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 1.9995 - accuracy: 0.3366\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 434us/step - loss: 1.2414 - accuracy: 0.6801\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.9079 - accuracy: 0.7997\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.7226 - accuracy: 0.8380\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.5839 - accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 0.5181 - accuracy: 0.8846\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.4560 - accuracy: 0.8978\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.4111 - accuracy: 0.9054\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 0.3674 - accuracy: 0.9186\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.3389 - accuracy: 0.9179\n",
      "12/12 [==============================] - 0s 373us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 1.8724 - accuracy: 0.3713\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 440us/step - loss: 1.2332 - accuracy: 0.6634\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 440us/step - loss: 0.9379 - accuracy: 0.7538\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 458us/step - loss: 0.7569 - accuracy: 0.8102\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.6247 - accuracy: 0.8456\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 0.5469 - accuracy: 0.8651\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 456us/step - loss: 0.4642 - accuracy: 0.8922\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 0.4301 - accuracy: 0.8957\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.3869 - accuracy: 0.9026\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 500us/step - loss: 0.3593 - accuracy: 0.9033\n",
      "12/12 [==============================] - 0s 454us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 545us/step - loss: 1.9753 - accuracy: 0.3261\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 1.2422 - accuracy: 0.6606\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.9343 - accuracy: 0.7580\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 0.7463 - accuracy: 0.8310\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.6209 - accuracy: 0.8554\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 0.5408 - accuracy: 0.8769\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 442us/step - loss: 0.4606 - accuracy: 0.8950\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.4053 - accuracy: 0.9103\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 0.3939 - accuracy: 0.9047\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.3372 - accuracy: 0.9277\n",
      "12/12 [==============================] - 0s 363us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 1.7542 - accuracy: 0.4433\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.8632 - accuracy: 0.7996\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 394us/step - loss: 0.5829 - accuracy: 0.8553\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 434us/step - loss: 0.4173 - accuracy: 0.8977\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 398us/step - loss: 0.3303 - accuracy: 0.9207\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 0.2750 - accuracy: 0.9276\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 0.2475 - accuracy: 0.9332\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 0.2332 - accuracy: 0.9353\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 393us/step - loss: 0.2029 - accuracy: 0.9429\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 0.1738 - accuracy: 0.9589\n",
      "12/12 [==============================] - 0s 371us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 488us/step - loss: 1.7006 - accuracy: 0.4266\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 0.8467 - accuracy: 0.7787\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 487us/step - loss: 0.5569 - accuracy: 0.8532\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 0.4193 - accuracy: 0.8914\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 0.3586 - accuracy: 0.8984\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 397us/step - loss: 0.3003 - accuracy: 0.9123\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 0.2511 - accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 390us/step - loss: 0.2317 - accuracy: 0.9304\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 0.2094 - accuracy: 0.9360\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.1909 - accuracy: 0.9464\n",
      "12/12 [==============================] - 0s 371us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 1.7832 - accuracy: 0.4152\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 0.9261 - accuracy: 0.7670\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 0.5682 - accuracy: 0.8644\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 0.4081 - accuracy: 0.8957\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 0.3347 - accuracy: 0.9096\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 396us/step - loss: 0.2451 - accuracy: 0.9395\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 421us/step - loss: 0.2323 - accuracy: 0.9465\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 0.2142 - accuracy: 0.9388\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 0.1764 - accuracy: 0.9485\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 399us/step - loss: 0.1692 - accuracy: 0.9548\n",
      "12/12 [==============================] - 0s 509us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 488us/step - loss: 1.6939 - accuracy: 0.4555\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.8500 - accuracy: 0.7698\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 410us/step - loss: 0.5523 - accuracy: 0.8547\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 444us/step - loss: 0.4115 - accuracy: 0.8915\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.3345 - accuracy: 0.9089\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 0.2874 - accuracy: 0.9221\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.2450 - accuracy: 0.9367\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 0.2204 - accuracy: 0.9388\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.1876 - accuracy: 0.9506\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 0.1793 - accuracy: 0.9471\n",
      "12/12 [==============================] - 0s 355us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 562us/step - loss: 1.7480 - accuracy: 0.4131\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 0.9491 - accuracy: 0.7455\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.6526 - accuracy: 0.8296\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 426us/step - loss: 0.5056 - accuracy: 0.8609\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 444us/step - loss: 0.3864 - accuracy: 0.8943\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 0.3268 - accuracy: 0.9096\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 397us/step - loss: 0.2830 - accuracy: 0.9318\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 402us/step - loss: 0.2472 - accuracy: 0.9312\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.2182 - accuracy: 0.9402\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 394us/step - loss: 0.2003 - accuracy: 0.9520\n",
      "12/12 [==============================] - 0s 384us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 673us/step - loss: 1.4072 - accuracy: 0.5581\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 436us/step - loss: 0.4771 - accuracy: 0.8880\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 0.3314 - accuracy: 0.9109\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 443us/step - loss: 0.2442 - accuracy: 0.9395\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.1954 - accuracy: 0.9520\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 0.1653 - accuracy: 0.9555\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 0.1305 - accuracy: 0.9673\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.1249 - accuracy: 0.9694\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 0.1096 - accuracy: 0.9680\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 459us/step - loss: 0.0981 - accuracy: 0.9715\n",
      "12/12 [==============================] - 0s 385us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 497us/step - loss: 1.4620 - accuracy: 0.5261\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 438us/step - loss: 0.5526 - accuracy: 0.8469\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.3396 - accuracy: 0.9081\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 0.2656 - accuracy: 0.9283\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.2080 - accuracy: 0.9436\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 0.1719 - accuracy: 0.9520\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 0.1464 - accuracy: 0.9596\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 455us/step - loss: 0.1234 - accuracy: 0.9652\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.1084 - accuracy: 0.9743\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.1022 - accuracy: 0.9763\n",
      "12/12 [==============================] - 0s 389us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 502us/step - loss: 1.4204 - accuracy: 0.5640\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 432us/step - loss: 0.4881 - accuracy: 0.8818\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 465us/step - loss: 0.3021 - accuracy: 0.9186\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.2154 - accuracy: 0.9465\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 457us/step - loss: 0.1921 - accuracy: 0.9478\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.1543 - accuracy: 0.9583\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 450us/step - loss: 0.1347 - accuracy: 0.9680\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 0.1161 - accuracy: 0.9680\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 0.1060 - accuracy: 0.9757\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 0.0906 - accuracy: 0.9764\n",
      "12/12 [==============================] - 0s 379us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 485us/step - loss: 1.3462 - accuracy: 0.5883\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 0.4981 - accuracy: 0.8672\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.3165 - accuracy: 0.9159\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 0.2297 - accuracy: 0.9402\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 525us/step - loss: 0.1926 - accuracy: 0.9465\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 480us/step - loss: 0.1631 - accuracy: 0.9590\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 446us/step - loss: 0.1456 - accuracy: 0.9604\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 0.1340 - accuracy: 0.9652\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 0.1097 - accuracy: 0.9701\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 0.1105 - accuracy: 0.9680\n",
      "12/12 [==============================] - 0s 366us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 1.3833 - accuracy: 0.5675\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 440us/step - loss: 0.5046 - accuracy: 0.8720\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 0.3092 - accuracy: 0.9186\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 449us/step - loss: 0.2386 - accuracy: 0.9374\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 463us/step - loss: 0.1861 - accuracy: 0.9478\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 0.1613 - accuracy: 0.9569\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 448us/step - loss: 0.1450 - accuracy: 0.9590\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 0.1352 - accuracy: 0.9708\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 445us/step - loss: 0.1041 - accuracy: 0.9757\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.1076 - accuracy: 0.9694\n",
      "12/12 [==============================] - 0s 378us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 459us/step - loss: 2.4928 - accuracy: 0.1294\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 2.2294 - accuracy: 0.2018\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 2.0208 - accuracy: 0.2721\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 1.9241 - accuracy: 0.3201\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 424us/step - loss: 1.7990 - accuracy: 0.3549\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 1.7054 - accuracy: 0.3981\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.5987 - accuracy: 0.4482\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 1.5946 - accuracy: 0.4342\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.4785 - accuracy: 0.4809\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.4465 - accuracy: 0.5073\n",
      "12/12 [==============================] - 0s 365us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 2.4613 - accuracy: 0.1392\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.1304 - accuracy: 0.2457\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 421us/step - loss: 1.9790 - accuracy: 0.2916\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.8769 - accuracy: 0.3229\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 423us/step - loss: 1.7547 - accuracy: 0.3681\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 406us/step - loss: 1.6753 - accuracy: 0.4203\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 427us/step - loss: 1.5683 - accuracy: 0.4697\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.5396 - accuracy: 0.4697\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 1.4693 - accuracy: 0.4830\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 1.3669 - accuracy: 0.5233\n",
      "12/12 [==============================] - 0s 363us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 2.4621 - accuracy: 0.1349\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 2.1726 - accuracy: 0.2260\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.0212 - accuracy: 0.2775\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 1.8298 - accuracy: 0.3303\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 426us/step - loss: 1.7008 - accuracy: 0.3880\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.6219 - accuracy: 0.4325\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.5342 - accuracy: 0.4569\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 412us/step - loss: 1.4701 - accuracy: 0.4708\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.3629 - accuracy: 0.5229\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 447us/step - loss: 1.3267 - accuracy: 0.5542\n",
      "12/12 [==============================] - 0s 357us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 447us/step - loss: 2.4301 - accuracy: 0.1537\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 2.1761 - accuracy: 0.2204\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 2.0009 - accuracy: 0.2698\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.8507 - accuracy: 0.3408\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 411us/step - loss: 1.6965 - accuracy: 0.4040\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 1.6020 - accuracy: 0.4395\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 408us/step - loss: 1.5299 - accuracy: 0.4694\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.4719 - accuracy: 0.4986\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.4273 - accuracy: 0.5042\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 1.3496 - accuracy: 0.5348\n",
      "12/12 [==============================] - 0s 389us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 2.4199 - accuracy: 0.1599\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 2.1619 - accuracy: 0.2344\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 1.9107 - accuracy: 0.3178\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.8094 - accuracy: 0.3700\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.6694 - accuracy: 0.4186\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.5861 - accuracy: 0.4416\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 435us/step - loss: 1.4971 - accuracy: 0.4791\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.4168 - accuracy: 0.5167\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 407us/step - loss: 1.4055 - accuracy: 0.5083\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 1.3706 - accuracy: 0.5160\n",
      "12/12 [==============================] - 0s 353us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 538us/step - loss: 2.4765 - accuracy: 0.1545\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 438us/step - loss: 1.9611 - accuracy: 0.3159\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 496us/step - loss: 1.6349 - accuracy: 0.4356\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.4439 - accuracy: 0.5157\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 1.3125 - accuracy: 0.5518\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 1.1685 - accuracy: 0.6075\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 485us/step - loss: 1.0672 - accuracy: 0.6493\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 487us/step - loss: 1.0261 - accuracy: 0.6576\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.8960 - accuracy: 0.7077\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 0.8658 - accuracy: 0.7244\n",
      "12/12 [==============================] - 0s 473us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 504us/step - loss: 2.4643 - accuracy: 0.1587\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 1.9444 - accuracy: 0.3097\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 486us/step - loss: 1.6789 - accuracy: 0.4259\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 1.4447 - accuracy: 0.5122\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.3045 - accuracy: 0.5560\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 1.1731 - accuracy: 0.6166\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 1.1016 - accuracy: 0.6395\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 480us/step - loss: 1.0191 - accuracy: 0.6597\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 0.9720 - accuracy: 0.6820\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 0.8905 - accuracy: 0.7203\n",
      "12/12 [==============================] - 0s 402us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 523us/step - loss: 2.5027 - accuracy: 0.1599\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 1.9051 - accuracy: 0.3136\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 1.6210 - accuracy: 0.4346\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.4031 - accuracy: 0.5111\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.2402 - accuracy: 0.5723\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 1.0812 - accuracy: 0.6377\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 0.9821 - accuracy: 0.6620\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.9776 - accuracy: 0.6732\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 0.8820 - accuracy: 0.7156\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.8087 - accuracy: 0.7406\n",
      "12/12 [==============================] - 0s 354us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 512us/step - loss: 2.4325 - accuracy: 0.1690\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 453us/step - loss: 1.9457 - accuracy: 0.3088\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 482us/step - loss: 1.6017 - accuracy: 0.4465\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 1.3931 - accuracy: 0.5229\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 1.1981 - accuracy: 0.5994\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 1.0945 - accuracy: 0.6398\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 1.0193 - accuracy: 0.6732\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 481us/step - loss: 0.9602 - accuracy: 0.6690\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.9078 - accuracy: 0.7003\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.8234 - accuracy: 0.7267\n",
      "12/12 [==============================] - 0s 370us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 537us/step - loss: 2.3349 - accuracy: 0.1954\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 453us/step - loss: 1.8356 - accuracy: 0.3435\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 1.5236 - accuracy: 0.4659\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 1.3310 - accuracy: 0.5508\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 1.2427 - accuracy: 0.5744\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 1.1041 - accuracy: 0.6217\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 1.0034 - accuracy: 0.6544\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.9295 - accuracy: 0.6926\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.9069 - accuracy: 0.6871\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.8343 - accuracy: 0.7197\n",
      "12/12 [==============================] - 0s 371us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 2.2579 - accuracy: 0.2171\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.6459 - accuracy: 0.4022\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 1.2990 - accuracy: 0.5421\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 421us/step - loss: 1.1370 - accuracy: 0.6124\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.0136 - accuracy: 0.6514\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 400us/step - loss: 0.9062 - accuracy: 0.6855\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 0.8031 - accuracy: 0.7293\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 0.7613 - accuracy: 0.7453\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 0.7154 - accuracy: 0.7634\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 0.6769 - accuracy: 0.7766\n",
      "12/12 [==============================] - 0s 350us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 465us/step - loss: 2.2740 - accuracy: 0.2331\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 1.6121 - accuracy: 0.4412\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 422us/step - loss: 1.3611 - accuracy: 0.5164\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 1.1758 - accuracy: 0.5859\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 1.0148 - accuracy: 0.6562\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 0.9388 - accuracy: 0.6799\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 0.8569 - accuracy: 0.7244\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 0.7739 - accuracy: 0.7397\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 0.7349 - accuracy: 0.7641\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 0.6822 - accuracy: 0.7704\n",
      "12/12 [==============================] - 0s 362us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 454us/step - loss: 2.1935 - accuracy: 0.2225\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 403us/step - loss: 1.5969 - accuracy: 0.4318\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.2603 - accuracy: 0.5591\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 419us/step - loss: 1.0864 - accuracy: 0.6280\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 398us/step - loss: 0.9435 - accuracy: 0.6739\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 0.8804 - accuracy: 0.6898\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 414us/step - loss: 0.7792 - accuracy: 0.7413\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 0.7379 - accuracy: 0.7490\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 413us/step - loss: 0.6872 - accuracy: 0.7656\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 0.6645 - accuracy: 0.7879\n",
      "12/12 [==============================] - 0s 443us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 490us/step - loss: 2.2354 - accuracy: 0.2302\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 404us/step - loss: 1.5942 - accuracy: 0.4388\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 420us/step - loss: 1.2717 - accuracy: 0.5577\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.1442 - accuracy: 0.5855\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 398us/step - loss: 0.9770 - accuracy: 0.6739\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 0.8906 - accuracy: 0.6982\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 0.8357 - accuracy: 0.7184\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 418us/step - loss: 0.7476 - accuracy: 0.7469\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 405us/step - loss: 0.7212 - accuracy: 0.7552\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 0.6808 - accuracy: 0.7726\n",
      "12/12 [==============================] - 0s 400us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 461us/step - loss: 2.2858 - accuracy: 0.1954\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 423us/step - loss: 1.6669 - accuracy: 0.4200\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 1.3298 - accuracy: 0.5410\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 409us/step - loss: 1.1354 - accuracy: 0.6120\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 401us/step - loss: 1.0101 - accuracy: 0.6579\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 431us/step - loss: 0.9354 - accuracy: 0.6794\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 425us/step - loss: 0.8070 - accuracy: 0.7302\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 416us/step - loss: 0.7721 - accuracy: 0.7448\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 415us/step - loss: 0.7059 - accuracy: 0.7663\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 417us/step - loss: 0.6785 - accuracy: 0.7691\n",
      "12/12 [==============================] - 0s 358us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 503us/step - loss: 1.9621 - accuracy: 0.3354\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 451us/step - loss: 1.0899 - accuracy: 0.6367\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.8090 - accuracy: 0.7418\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 480us/step - loss: 0.6500 - accuracy: 0.7968\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.5968 - accuracy: 0.8017\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 475us/step - loss: 0.5060 - accuracy: 0.8441\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 0.4679 - accuracy: 0.8434\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 477us/step - loss: 0.4074 - accuracy: 0.8678\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.3611 - accuracy: 0.8900\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 473us/step - loss: 0.3559 - accuracy: 0.8866\n",
      "12/12 [==============================] - 0s 381us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 508us/step - loss: 1.9694 - accuracy: 0.3201\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 452us/step - loss: 1.1386 - accuracy: 0.6068\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 483us/step - loss: 0.8578 - accuracy: 0.7293\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 0.6481 - accuracy: 0.7891\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 0.5743 - accuracy: 0.8051\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 467us/step - loss: 0.4993 - accuracy: 0.8420\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.4487 - accuracy: 0.8504\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.3667 - accuracy: 0.8775\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.3802 - accuracy: 0.8810\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 460us/step - loss: 0.3629 - accuracy: 0.8838\n",
      "12/12 [==============================] - 0s 395us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 517us/step - loss: 2.0205 - accuracy: 0.3164\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 444us/step - loss: 1.0977 - accuracy: 0.6370\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 470us/step - loss: 0.7630 - accuracy: 0.7490\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 469us/step - loss: 0.6576 - accuracy: 0.7837\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.5438 - accuracy: 0.8192\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 471us/step - loss: 0.4433 - accuracy: 0.8623\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 472us/step - loss: 0.4181 - accuracy: 0.8672\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 457us/step - loss: 0.3594 - accuracy: 0.8790\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.3401 - accuracy: 0.8776\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 487us/step - loss: 0.3111 - accuracy: 0.8950\n",
      "12/12 [==============================] - 0s 374us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 516us/step - loss: 2.0016 - accuracy: 0.3234\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 457us/step - loss: 1.0989 - accuracy: 0.6266\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 476us/step - loss: 0.8684 - accuracy: 0.7191\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 485us/step - loss: 0.6790 - accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 478us/step - loss: 0.5587 - accuracy: 0.8241\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 479us/step - loss: 0.5275 - accuracy: 0.8324\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 474us/step - loss: 0.4508 - accuracy: 0.8512\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 457us/step - loss: 0.4169 - accuracy: 0.8665\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 464us/step - loss: 0.3924 - accuracy: 0.8769\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 494us/step - loss: 0.3827 - accuracy: 0.8839\n",
      "12/12 [==============================] - 0s 394us/step\n",
      "Epoch 1/10\n",
      "45/45 [==============================] - 0s 580us/step - loss: 1.9659 - accuracy: 0.3199\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 519us/step - loss: 1.1332 - accuracy: 0.6106\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 519us/step - loss: 0.8553 - accuracy: 0.7156\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 512us/step - loss: 0.6889 - accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 553us/step - loss: 0.5458 - accuracy: 0.8324\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 514us/step - loss: 0.5100 - accuracy: 0.8303\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 535us/step - loss: 0.4364 - accuracy: 0.8595\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 521us/step - loss: 0.3927 - accuracy: 0.8727\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 462us/step - loss: 0.3833 - accuracy: 0.8860\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 468us/step - loss: 0.3463 - accuracy: 0.8936\n",
      "12/12 [==============================] - 0s 371us/step\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 0s 475us/step - loss: 1.8846 - accuracy: 0.3678\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 0s 428us/step - loss: 0.7422 - accuracy: 0.7696\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 0s 451us/step - loss: 0.4163 - accuracy: 0.8603\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 0s 449us/step - loss: 0.3170 - accuracy: 0.9021\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 0s 445us/step - loss: 0.2852 - accuracy: 0.9188\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 0s 445us/step - loss: 0.1913 - accuracy: 0.9382\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 0s 444us/step - loss: 0.1551 - accuracy: 0.9538\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 0s 463us/step - loss: 0.1603 - accuracy: 0.9560\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 0s 445us/step - loss: 0.1603 - accuracy: 0.9482\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 0s 442us/step - loss: 0.1196 - accuracy: 0.9583\n",
      "Best: 0.962711 using {'activation': 'relu', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.861427 (0.015958) with: {'activation': 'relu', 'dropout_rate': 0.2, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.908731 (0.014812) with: {'activation': 'relu', 'dropout_rate': 0.2, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.954362 (0.012160) with: {'activation': 'relu', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.962711 (0.007818) with: {'activation': 'relu', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.602129 (0.053466) with: {'activation': 'relu', 'dropout_rate': 0.5, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.833592 (0.020177) with: {'activation': 'relu', 'dropout_rate': 0.5, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.892035 (0.027267) with: {'activation': 'relu', 'dropout_rate': 0.5, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.938223 (0.010952) with: {'activation': 'relu', 'dropout_rate': 0.5, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.903154 (0.026906) with: {'activation': 'tanh', 'dropout_rate': 0.2, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.940455 (0.008913) with: {'activation': 'tanh', 'dropout_rate': 0.2, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.956589 (0.009601) with: {'activation': 'tanh', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.962153 (0.007412) with: {'activation': 'tanh', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.851987 (0.026725) with: {'activation': 'tanh', 'dropout_rate': 0.5, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.906507 (0.014178) with: {'activation': 'tanh', 'dropout_rate': 0.5, 'momentum': 0.5, 'num_layers': 3, 'num_neurons': 64}\n",
      "0.929876 (0.018866) with: {'activation': 'tanh', 'dropout_rate': 0.5, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 32}\n",
      "0.959369 (0.008768) with: {'activation': 'tanh', 'dropout_rate': 0.5, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64}\n",
      "   Num Layers Num Neurons Activation Dropout Rate Momentum  Mean Accuracy  \\\n",
      "0           3          32       relu          0.2      0.5       0.861427   \n",
      "1           3          64       relu          0.2      0.5       0.908731   \n",
      "2           3          32       relu          0.2      0.9       0.954362   \n",
      "3           3          64       relu          0.2      0.9       0.962711   \n",
      "4           3          32       relu          0.5      0.5       0.602129   \n",
      "5           3          64       relu          0.5      0.5       0.833592   \n",
      "6           3          32       relu          0.5      0.9       0.892035   \n",
      "7           3          64       relu          0.5      0.9       0.938223   \n",
      "8           3          32       tanh          0.2      0.5       0.903154   \n",
      "9           3          64       tanh          0.2      0.5       0.940455   \n",
      "10          3          32       tanh          0.2      0.9       0.956589   \n",
      "11          3          64       tanh          0.2      0.9       0.962153   \n",
      "12          3          32       tanh          0.5      0.5       0.851987   \n",
      "13          3          64       tanh          0.5      0.5       0.906507   \n",
      "14          3          32       tanh          0.5      0.9       0.929876   \n",
      "15          3          64       tanh          0.5      0.9       0.959369   \n",
      "\n",
      "    Std Accuracy  \n",
      "0       0.015958  \n",
      "1       0.014812  \n",
      "2       0.012160  \n",
      "3       0.007818  \n",
      "4       0.053466  \n",
      "5       0.020177  \n",
      "6       0.027267  \n",
      "7       0.010952  \n",
      "8       0.026906  \n",
      "9       0.008913  \n",
      "10      0.009601  \n",
      "11      0.007412  \n",
      "12      0.026725  \n",
      "13      0.014178  \n",
      "14      0.018866  \n",
      "15      0.008768  \n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.12.0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import SGD\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define create_model function for KerasClassifier\n",
    "def create_model(num_layers=1, num_neurons=64, activation='relu', dropout_rate=0.0, momentum=0.9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=X_scaled.shape[1], activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    optimizer = SGD(learning_rate=0.01, momentum=momentum)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define parameters grid for grid search\n",
    "param_grid = {\n",
    "    'num_layers': [3],\n",
    "    'num_neurons': [32, 64],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'momentum': [0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Create KerasClassifier wrapper for scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=kfold, scoring='accuracy')\n",
    "grid_result = grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, std, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, std, param))\n",
    "results_df = pd.DataFrame(grid_result.cv_results_)\n",
    "\n",
    "# Select relevant columns (you can adjust this based on what you need)\n",
    "results_df = results_df[['param_num_layers', 'param_num_neurons', 'param_activation', \n",
    "                         'param_dropout_rate', 'param_momentum', \n",
    "                         'mean_test_score', 'std_test_score']]\n",
    "\n",
    "# Rename columns for clarity\n",
    "results_df.columns = ['Num Layers', 'Num Neurons', 'Activation', 'Dropout Rate', \n",
    "                      'Momentum', 'Mean Accuracy', 'Std Accuracy']\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8452a7-8862-4839-8257-a18544af862b",
   "metadata": {},
   "source": [
    "Based on accuracy, the model with the parameters of {'activation': 'relu', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64} is the best, with 0.962711 accuracy. However, it is only the second most consistent, with the  standard deviation of 0.007818. The most consistent model ekes it out with an accuracy std. of 0.007412 and an accuracy of 0.962153. Its parameters are {'activation': 'tanh', 'dropout_rate': 0.2, 'momentum': 0.9, 'num_layers': 3, 'num_neurons': 64}-- very similar, just with a differing activation function. The \"best model\" between these two depends on use case, as while both are very good, one is more accurate and the other is more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74747b71-98da-45c6-897e-99ee0d4f238c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
