{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4229e7ec-f5ea-48a4-a33c-327ff7eb6c8b",
   "metadata": {},
   "source": [
    "# Abstract/Background Summary \n",
    "\n",
    "Neural population activity: rich spatiotemporal dynamic patterns which can be modelled more efficiently w/ lower-dimension latent factors, which may reveal things not evident in high-dimensional or single-unit (?) activities.\n",
    "\n",
    "For accuracy: capture potential nonlinearities, have data-efficient architecture for generalization, capable of causal/real-time and non-causal inference simultaneously, and infer even w/ missing neural measurements. \n",
    "\n",
    "Current models are often Lienar Dynamical Models: infer low-dimension latent factors (colinear relationships???) or build brain-machine interfaces (BMIs). \n",
    "\n",
    "Pros: Data-efficient to train & allow for flexible inference w/ Kalman filtering\n",
    "Cons: can't capture non-linearitys -> not as accurate\n",
    "\n",
    "What about Deep Learning? \n",
    "pros: capture nonlinearities\n",
    "cons: no flexible inference, not solvable analytically and thus need to train inference/recognition network simultaneously w/ generative network (need entire length of data over trial). Inference thus depends on how specific network structure & is not flexible. \n",
    "\n",
    "Sequential Autoencoders (SAE) or LDM w/ nonlinear embeddings: inference is non-causal: real time + recursive inference not addressed. Does not have inference in case of missing observations. \n",
    "\n",
    "Predictive dynamical models using forward recurrent neural networks (RNNs) don't emable flexible inference. Allow for causal inference, but no non-causal inference to leverage al data, and don't address mising observations. \n",
    "\n",
    "DFINE: two sets of latent factors: \n",
    "dynamic factors = how neural population activiy evolves over low-dim non-linear manifold\n",
    "manifold factors = characterize how maiforld is embedded in high-dimensional neural activity space\n",
    "\n",
    "two sets --> capture nonlinearity in manifold factors while keeping linear dynamics on linear== exploit Kalman filteron nonlinear manifold.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8186029-d71f-4fbf-8e7e-2bad04ea1239",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "dynamic latent factors: temporal dynamics on nonlinear mainfold\n",
    "manifold latent factors: describe low-dim manifold embedded in high-dim neural population activity space\n",
    "\n",
    "capture nonlinearity w/ link between mainfold factors and neural population activity while keeping manifold dynamics linear. \n",
    "\n",
    "Flexible inference: both causal (Kalman filtering) and non causal (Kalman smoothing) inference as well as inferring missing observations\n",
    "Inferecen is recursive: current inferred factor can be used to get next inferred factor w/o need to reprocess neural data --> coputational efficiency and real-time implementations. \n",
    "\n",
    "manifold latent factors: lower-dimensional representation of neural population activity, mapped w/ decoder and encoder networks of multi-layer perceptions (universal approximators of any nonlinear function under mild conditions)\n",
    "\n",
    "Dynamic and manifold latnet factors form a LDM: \n",
    "- manifld latent factors = noisy observations from dynamic latent factors constituting linear dynamic model states.\n",
    "- time evolution dscrived through linear dynamic model with additive noise (gaussian)\n",
    "- use backpropagation to learn model parameters by minimizing root mean-squared error (RMSE) of predicted future neural observations from past neural observations\n",
    "\n",
    "- dynamic and manifold latent factors are learned together in end-to-end gradient-descent optimization (?)\n",
    "   DFINE learns best nonlinear manifold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c90a43-3f4a-4c41-a057-33d303c1ba9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533be17-444f-43b1-8d5e-8d53ac8b2624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4414fa76-9a57-4df4-9295-936934e0377e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1624ff9-e9b6-4374-b083-e9ad96a9b98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95e4ba-212b-4f18-a54d-c051e1c9ebc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9c6ff-724b-4431-a95f-c5c722680378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b24244-16a3-41ea-96fb-2588f4d9e7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded37696-b245-455e-9e8b-1f50a2886449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f887e49-2d1a-4b04-89b5-e9a0d0af117c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2e6af-f762-4e3e-a893-aa451a124a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
