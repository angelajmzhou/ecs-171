{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637620a3",
   "metadata": {},
   "source": [
    "Lab3: This lab activity has 3 parts. Use the provided information below to answer the questions. \n",
    "\n",
    "![Generated Dataset](lab3.jpg)\n",
    "![Generated Dataset](lab3-1.jpg)\n",
    "![Generated Dataset](lab3-2.jpg)\n",
    "![Generated Dataset](lab3-3.jpg)\n",
    "- Part1) Mathematically provide the formulation for performing a Full-pass for the first mini-batch of data. \n",
    "- Part2) Provide the mathematical formulation and pseudo-computation for the weight changes at each layer of the model during backpropagation and indicate the notations for the updated weights. \n",
    "- Part3) Assume you have completed 1 epoch of training. How do you compute the error of your model after completing 1 epoch on training and testing data (separately)? Provide a pseudocode to describe your algorithm for reporting the training and testing error after each epoch to investigate when the model converges (after how many epochs). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d7457-8718-4986-aae4-eb66414592db",
   "metadata": {},
   "source": [
    "Part 1\n",
    "--\n",
    "To perform a full-pass on the first mini-batch of data:\n",
    "\n",
    "**general notation :**\n",
    "\n",
    "$h_1$: hidden layer 1\n",
    "\n",
    "$h_2$: hidden layer 2\n",
    "\n",
    "$N$ : number of datapoints\n",
    "\n",
    "$n$ : number of batches\n",
    "\n",
    "$o$: output layer ($o_1 = N_5$, $o_2 = N_6$, $o_3 = N_7$)\n",
    "\n",
    "$w_{ij, k}$: weights connecting layer $i$ to layer $j$ for node number $k$\n",
    "\n",
    "forward pass:\n",
    "---\n",
    "for each batch, take N\\n points of the dataset and perform the following steps for the forward pass. This will happen n times for the n batches.\n",
    "\n",
    "**input to hidden layer one:**\n",
    "$$\n",
    "h_{1,1} = \\sum_{i=1} ^ 2 w_{ih1, 1} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_{1,2} = \\sum_{i=1} ^ 2 w_{ih1, 2} x_i\n",
    "$$\n",
    "\n",
    "**h1 activation function**\n",
    "$$\n",
    "a_{1,1} = \\sigma(h_{1,1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_{1,2} = \\sigma(h_{1,2})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**hidder layer one to hidden layer two:**\n",
    "$$\n",
    "h_{2,1} = \\sum_{i=1} ^ 2 w_{h1h2, 3} a_{1, i}\n",
    "$$\n",
    "$$\n",
    "h_{2,2} = \\sum_{i=1} ^ 2 w_{h1h2, 4} a_{1, i}\n",
    "$$\n",
    "\n",
    "**h2 activation function**\n",
    "$$\n",
    "a_{2,1} = \\sigma(h_{2,1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_{2,2} = \\sigma(h_{2,2})\n",
    "$$\n",
    "\n",
    "**h2 to output layer:**\n",
    "$$\n",
    "o_1  = \\sum_{i=1} ^ 2 w_{h_2o, 5} h_{2, i}\n",
    "$$\n",
    "$$\n",
    "o_2  = \\sum_{i=1} ^ 2 w_{h_2o, 6} h_{2, i}\n",
    "$$\n",
    "$$\n",
    "o_3  = \\sum_{i=1} ^ 2 w_{h_2o, 7} h_{2, i}\n",
    "$$\n",
    "\n",
    "**output layer activation function**\n",
    "$$\n",
    "y_1 = \\sigma(o_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 = \\sigma(o_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_3 = \\sigma(o_3)\n",
    "$$\n",
    "\n",
    "\n",
    "backpropagation:\n",
    "---\n",
    "given that a *sigmoid* activation function is used:\n",
    "We define $E_n(w)$ as the error function -- consisting of the error of $y_1$, $y_2$, and $y_3$ combined.\n",
    "Alternatively, define error as: $\\sum_{i=1}^3 (\\hat{y_i} - y_i)$ for each individual pass, or the average error per batch as: $\\frac{1}{N/n}\\sum_{j=1}^{N/n} \\frac{1}{3}\\sum_{i=1}^3 (\\hat{y_i} - y_i)$\n",
    "\n",
    "\n",
    "\n",
    "1. **Output Layer Gradients:**\n",
    "\n",
    "for last layer of weights before output. note that $o_n$ designates the function of the weights connecting hidden layer 2 to the output layer, i.e. $o_n  = \\sum_{i=1} ^ 2 w_{h_2o, n} h_{2, i}$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{h_2o}} = \\frac{\\partial E_n(w)}{\\partial y_n} \\times \\frac{\\partial y_n}{\\partial o_n} \\times \\frac{\\partial o_n}{\\partial w_{h_{2o}}}\n",
    "$$\n",
    "\n",
    "   Using the chain rule:\n",
    "   This operation is performed once per output node, then averaged for all three outputs. That itself is also looped N/n times for the number of datapoints in each batch to get an error term for mini-batch gradient descent.\n",
    "   $$\n",
    "   \\frac{\\partial E_n(w)}{\\partial w_{h_2o}} = (y - \\hat{y})\\times \\frac{\\partial y_n}{\\partial o_n}  \\times \\frac{\\partial o_n}{\\partial w_{h_2o}} \n",
    "   $$\n",
    "\n",
    "   \n",
    "   $$\n",
    "   = (y - \\hat{y})\\times (\\sigma(o_n)(1-\\sigma (o_n)) \\times \\frac{\\partial o_n}{\\partial w_{h_2o}}\n",
    "   $$\n",
    "    remember...\n",
    "   $\\hat{y} = \\sigma(o_n)$ and $o_n  = \\sum_{i=1} ^ 2 w_{h_2o, n} h_{2, i}$\n",
    "\n",
    "   of course, this is all given with the sigmoid function and its derivative:\n",
    "   \n",
    "$Sigmoid = \\sigma (x) = 1/(1+e^{-x})$\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial \\sigma (x)}{\\partial x} = \\sigma(x) (1- \\sigma (x))\n",
    "   $$\n",
    "\n",
    "3. **Hidden Layer 2 Gradients:**\n",
    "\n",
    "continuing with two more layers of derivatives to get the gradient w.r.t. the weights connecting hidden layer 1 to hidden layer 2. m = 3 or 4, representing N3 or N4 depending on which weight we're updating. Here, $o_n$ just represents the generic function taking us from hidden layer 2 to the output layer. \n",
    "\n",
    " $$\n",
    "   \\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{h_1h_2}} = \\frac{\\partial E_n(w)}{\\partial y_{n}} \\times \\frac{\\partial y_n}{\\partial o_n} \\times \\frac{\\partial o_n}{\\partial a_{2, m}} \\times \\frac{\\partial a_{2, m}}{\\partial h_{2,m}} \\times \\frac{\\partial h_{2,m}}{\\partial w_{h_1h_2, m}}\n",
    "   $$\n",
    "   \n",
    "Using the chain rule:\n",
    "\n",
    "   $$\n",
    "    \\frac{\\partial E_n(w)}{\\partial w_{h_1h_2}} = (y-\\hat{y}) \\times \\frac{\\partial y_n}{\\partial o_n} \\times \\frac{\\partial o_n}{\\partial a_{2, m}} \\times \\frac{\\partial a_{2, m}}{\\partial h_{2,m}} \\times \\frac{\\partial h_{2,m}}{\\partial w_{h_1h_2, m}}\n",
    "  = (y - \\hat{y})\\times (\\sigma(o_n)(1-\\sigma (o_n)) \\times \\frac{\\partial o_n}{\\partial a_{2, m}} \\times \\frac{\\partial a_{2, m}}{\\partial h_{2,m}} \\times \\frac{\\partial h_{2,m}}{\\partial w_{h_1h_2, m}}\n",
    "   $$\n",
    "\n",
    "4. **Hidden Layer 1 Gradients:**\n",
    "continuing with an additional two more layers of derivatives to get the gradient w.r.t. the weights connecting inputs to hidden layer 1. k = 1 or 2, representing N1 and N2 depending on which weight we're updating. Here, $o_n$ just represents the generic function taking us from hidden layer 2 to the output layer, and $h_{2,m}$ represents the generic function taking us from hidden layer 1 to hidden layer 2.\n",
    "\n",
    "\n",
    "\n",
    "    $$\n",
    "   \\Delta w = \\frac{\\partial E_n(w)}{\\partial w_{h_1h_2}} = \\frac{\\partial E_n(w)}{\\partial y_{n}} \\times \\frac{\\partial y_n}{\\partial o_n} \\times \\frac{\\partial o_n}{\\partial a_{2, m}} \\times \\frac{\\partial a_{2, m}}{\\partial h_{2,m}} \\times \\frac{\\partial h_{2,m}}{\\partial a_{1,k}}\\frac{\\partial a_{1, k}}{\\partial h_{1, k}} \\times \\frac{\\partial h_{1,k}}{\\partial w_{ih_1, k}}\n",
    "   $$\n",
    "   \n",
    "Using the chain rule:\n",
    "\n",
    "$$   \n",
    "     \\frac{\\partial E_n(w)}{\\partial w_{h_1h_2}} = (y-\\hat{y}) \\times \\frac{\\partial y_n}{\\partial o_n} \\times \\frac{\\partial o_n}{\\partial a_{2, m}} \\times \\frac{\\partial a_{2, m}}{\\partial h_{2,m}} \\times \\frac{\\partial h_{2,m}}{\\partial a_{1,k}}\\times\\frac{\\partial a_{1, k}}{\\partial h_{1, k}} \\times \\frac{\\partial h_{1,k}}{\\partial w_{ih_1, k}}\n",
    "     $$\n",
    "\n",
    "$$\n",
    "  = (y - \\hat{y})\\times (\\sigma(o_n)(1-\\sigma (o_n)) \\times \\frac{\\partial o_n}{\\partial a_{2, m}} \\times \\frac{\\partial a_{2, m}}{\\partial h_{2,m}} \\times \\frac{\\partial h_{2,m}}{\\partial a_{1,k}}\\times\\frac{\\partial a_{1, k}}{\\partial h_{1, k}} \\times \\frac{\\partial h_{1,k}}{\\partial w_{ih_1, k}}\n",
    "   $$\n",
    "\n",
    "Part 2\n",
    "--\n",
    "\n",
    "### Weight and Bias Updates\n",
    "\n",
    "Using a learning rate $ \\alpha $, update each layerâ€™s weights and biases:\n",
    "$$\n",
    "W_{h_2o} := W_{h_2o} - \\alpha \\frac{\\partial  E_n(w)}{\\partial W_{h_2o}}\n",
    "$$\n",
    "$$\n",
    "W_{h_1 h_2} := W_{h_1 h_2} - \\alpha \\frac{\\partial  E_n(w)}{\\partial W_{h_1 h_2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{i h_1} := W_{i h_1} - \\alpha \\frac{\\partial  E_n(w)}{\\partial W_{i h_1}}\n",
    "$$\n",
    "\n",
    "\n",
    "Part 3\n",
    "--\n",
    "\n",
    "- Part3) Assume you have completed 1 epoch of training. How do you compute the error of your model after completing 1 epoch on training and testing data (separately)? Provide a pseudocode to describe your algorithm for reporting the training and testing error after each epoch to investigate when the model converges (after how many epochs). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b31d834-9ecd-45fd-a8ea-19d814f4462c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m, x_test\n\u001b[1;32m      2\u001b[0m testing_data \u001b[38;5;241m=\u001b[39m y_train, y_test  \u001b[38;5;66;03m# Testing dataset with input-output pairs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to calculate SSE for a dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "training_data = x_train, y_train\n",
    "testing_data = x_test, y_test  \n",
    "\n",
    "\n",
    "def main():\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_sse = 0\n",
    "        for batch in n\n",
    "            predictions = forward_pass(x_train[batch])\n",
    "            backprop(predictions, y_train[batch]) #backprop, update weights\n",
    "            train_sse += calculate_sse(predictions, y_train[batch])\n",
    "    \n",
    "        test_pred = forward_pass(x_test)\n",
    "        testing_sse = calculate_sse(test_pred, y_test)\n",
    "        train_sse = train_sse/n \n",
    "        if test_sse < condition:\n",
    "            print(\"training complete, took \"+epoch+\" epochs.\")\n",
    "            break\n",
    "\n",
    "\n",
    "#function that calculates SSE\n",
    "def calculate_sse(y, y_pred):\n",
    "    sse = 0\n",
    "    for (x, y) in data:\n",
    "        for y_pred in y_predictions:\n",
    "            sse += sum((y - y_pred)^2 \n",
    "    return sse / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804b6b6-bace-4fb6-a7bc-a0ec1917a74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
